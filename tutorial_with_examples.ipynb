{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "\n",
    "Learning with self-contained examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 31326668.000000\n",
      "Loss at epoch 1: 32817700.000000\n",
      "Loss at epoch 2: 41571508.000000\n",
      "Loss at epoch 3: 49915588.000000\n",
      "Loss at epoch 4: 46967600.000000\n",
      "Loss at epoch 5: 30265800.000000\n",
      "Loss at epoch 6: 13298404.000000\n",
      "Loss at epoch 7: 4960097.000000\n",
      "Loss at epoch 8: 2200034.500000\n",
      "Loss at epoch 9: 1328253.375000\n",
      "Loss at epoch 10: 984668.562500\n",
      "Loss at epoch 11: 796770.437500\n",
      "Loss at epoch 12: 666703.062500\n",
      "Loss at epoch 13: 566347.312500\n",
      "Loss at epoch 14: 485390.906250\n",
      "Loss at epoch 15: 418794.093750\n",
      "Loss at epoch 16: 363340.375000\n",
      "Loss at epoch 17: 316857.312500\n",
      "Loss at epoch 18: 277654.156250\n",
      "Loss at epoch 19: 244298.734375\n",
      "Loss at epoch 20: 215763.156250\n",
      "Loss at epoch 21: 191215.953125\n",
      "Loss at epoch 22: 170003.656250\n",
      "Loss at epoch 23: 151595.531250\n",
      "Loss at epoch 24: 135629.484375\n",
      "Loss at epoch 25: 121680.085938\n",
      "Loss at epoch 26: 109438.171875\n",
      "Loss at epoch 27: 98637.984375\n",
      "Loss at epoch 28: 89085.796875\n",
      "Loss at epoch 29: 80627.343750\n",
      "Loss at epoch 30: 73102.242188\n",
      "Loss at epoch 31: 66393.687500\n",
      "Loss at epoch 32: 60395.601562\n",
      "Loss at epoch 33: 55020.312500\n",
      "Loss at epoch 34: 50193.937500\n",
      "Loss at epoch 35: 45849.425781\n",
      "Loss at epoch 36: 41933.617188\n",
      "Loss at epoch 37: 38398.394531\n",
      "Loss at epoch 38: 35197.933594\n",
      "Loss at epoch 39: 32297.306641\n",
      "Loss at epoch 40: 29663.035156\n",
      "Loss at epoch 41: 27267.142578\n",
      "Loss at epoch 42: 25086.085938\n",
      "Loss at epoch 43: 23097.953125\n",
      "Loss at epoch 44: 21283.664062\n",
      "Loss at epoch 45: 19626.550781\n",
      "Loss at epoch 46: 18110.695312\n",
      "Loss at epoch 47: 16723.976562\n",
      "Loss at epoch 48: 15452.977539\n",
      "Loss at epoch 49: 14288.379883\n",
      "Loss at epoch 50: 13218.968750\n",
      "Loss at epoch 51: 12236.927734\n",
      "Loss at epoch 52: 11333.741211\n",
      "Loss at epoch 53: 10502.628906\n",
      "Loss at epoch 54: 9737.633789\n",
      "Loss at epoch 55: 9032.884766\n",
      "Loss at epoch 56: 8383.455078\n",
      "Loss at epoch 57: 7784.244141\n",
      "Loss at epoch 58: 7231.343750\n",
      "Loss at epoch 59: 6721.351074\n",
      "Loss at epoch 60: 6250.095703\n",
      "Loss at epoch 61: 5814.602539\n",
      "Loss at epoch 62: 5411.527344\n",
      "Loss at epoch 63: 5038.436035\n",
      "Loss at epoch 64: 4692.645996\n",
      "Loss at epoch 65: 4372.185547\n",
      "Loss at epoch 66: 4075.253174\n",
      "Loss at epoch 67: 3799.798828\n",
      "Loss at epoch 68: 3544.098145\n",
      "Loss at epoch 69: 3306.633301\n",
      "Loss at epoch 70: 3086.050293\n",
      "Loss at epoch 71: 2881.080322\n",
      "Loss at epoch 72: 2690.606445\n",
      "Loss at epoch 73: 2513.451172\n",
      "Loss at epoch 74: 2348.641113\n",
      "Loss at epoch 75: 2195.300781\n",
      "Loss at epoch 76: 2052.642090\n",
      "Loss at epoch 77: 1919.745483\n",
      "Loss at epoch 78: 1796.005859\n",
      "Loss at epoch 79: 1680.624390\n",
      "Loss at epoch 80: 1573.075195\n",
      "Loss at epoch 81: 1472.781982\n",
      "Loss at epoch 82: 1379.263550\n",
      "Loss at epoch 83: 1291.962036\n",
      "Loss at epoch 84: 1210.534790\n",
      "Loss at epoch 85: 1134.514160\n",
      "Loss at epoch 86: 1063.492920\n",
      "Loss at epoch 87: 997.153503\n",
      "Loss at epoch 88: 935.211121\n",
      "Loss at epoch 89: 877.295959\n",
      "Loss at epoch 90: 823.123047\n",
      "Loss at epoch 91: 772.481323\n",
      "Loss at epoch 92: 725.102417\n",
      "Loss at epoch 93: 680.772827\n",
      "Loss at epoch 94: 639.279480\n",
      "Loss at epoch 95: 600.452759\n",
      "Loss at epoch 96: 564.099365\n",
      "Loss at epoch 97: 530.049194\n",
      "Loss at epoch 98: 498.146759\n",
      "Loss at epoch 99: 468.250427\n",
      "Loss at epoch 100: 440.253082\n",
      "Loss at epoch 101: 414.018646\n",
      "Loss at epoch 102: 389.422241\n",
      "Loss at epoch 103: 366.342194\n",
      "Loss at epoch 104: 344.696808\n",
      "Loss at epoch 105: 324.388397\n",
      "Loss at epoch 106: 305.338043\n",
      "Loss at epoch 107: 287.452301\n",
      "Loss at epoch 108: 270.665375\n",
      "Loss at epoch 109: 254.896530\n",
      "Loss at epoch 110: 240.095749\n",
      "Loss at epoch 111: 226.186996\n",
      "Loss at epoch 112: 213.117935\n",
      "Loss at epoch 113: 200.838898\n",
      "Loss at epoch 114: 189.297821\n",
      "Loss at epoch 115: 178.447601\n",
      "Loss at epoch 116: 168.245300\n",
      "Loss at epoch 117: 158.650497\n",
      "Loss at epoch 118: 149.629730\n",
      "Loss at epoch 119: 141.142685\n",
      "Loss at epoch 120: 133.159760\n",
      "Loss at epoch 121: 125.651672\n",
      "Loss at epoch 122: 118.582085\n",
      "Loss at epoch 123: 111.924706\n",
      "Loss at epoch 124: 105.657906\n",
      "Loss at epoch 125: 99.755318\n",
      "Loss at epoch 126: 94.196152\n",
      "Loss at epoch 127: 88.960213\n",
      "Loss at epoch 128: 84.027634\n",
      "Loss at epoch 129: 79.378502\n",
      "Loss at epoch 130: 74.996948\n",
      "Loss at epoch 131: 70.868713\n",
      "Loss at epoch 132: 66.976738\n",
      "Loss at epoch 133: 63.304432\n",
      "Loss at epoch 134: 59.844589\n",
      "Loss at epoch 135: 56.579350\n",
      "Loss at epoch 136: 53.498833\n",
      "Loss at epoch 137: 50.592976\n",
      "Loss at epoch 138: 47.850594\n",
      "Loss at epoch 139: 45.262280\n",
      "Loss at epoch 140: 42.820534\n",
      "Loss at epoch 141: 40.514984\n",
      "Loss at epoch 142: 38.337093\n",
      "Loss at epoch 143: 36.281872\n",
      "Loss at epoch 144: 34.340279\n",
      "Loss at epoch 145: 32.507137\n",
      "Loss at epoch 146: 30.775261\n",
      "Loss at epoch 147: 29.138807\n",
      "Loss at epoch 148: 27.592197\n",
      "Loss at epoch 149: 26.130503\n",
      "Loss at epoch 150: 24.749508\n",
      "Loss at epoch 151: 23.444046\n",
      "Loss at epoch 152: 22.210573\n",
      "Loss at epoch 153: 21.043911\n",
      "Loss at epoch 154: 19.940407\n",
      "Loss at epoch 155: 18.897367\n",
      "Loss at epoch 156: 17.909412\n",
      "Loss at epoch 157: 16.975882\n",
      "Loss at epoch 158: 16.092493\n",
      "Loss at epoch 159: 15.256340\n",
      "Loss at epoch 160: 14.465219\n",
      "Loss at epoch 161: 13.716855\n",
      "Loss at epoch 162: 13.008237\n",
      "Loss at epoch 163: 12.337210\n",
      "Loss at epoch 164: 11.702137\n",
      "Loss at epoch 165: 11.100908\n",
      "Loss at epoch 166: 10.531154\n",
      "Loss at epoch 167: 9.991797\n",
      "Loss at epoch 168: 9.481668\n",
      "Loss at epoch 169: 8.997642\n",
      "Loss at epoch 170: 8.539375\n",
      "Loss at epoch 171: 8.105225\n",
      "Loss at epoch 172: 7.693594\n",
      "Loss at epoch 173: 7.303823\n",
      "Loss at epoch 174: 6.933952\n",
      "Loss at epoch 175: 6.583783\n",
      "Loss at epoch 176: 6.251677\n",
      "Loss at epoch 177: 5.937105\n",
      "Loss at epoch 178: 5.638364\n",
      "Loss at epoch 179: 5.355528\n",
      "Loss at epoch 180: 5.087115\n",
      "Loss at epoch 181: 4.832570\n",
      "Loss at epoch 182: 4.591158\n",
      "Loss at epoch 183: 4.362136\n",
      "Loss at epoch 184: 4.144944\n",
      "Loss at epoch 185: 3.938652\n",
      "Loss at epoch 186: 3.743011\n",
      "Loss at epoch 187: 3.557377\n",
      "Loss at epoch 188: 3.381096\n",
      "Loss at epoch 189: 3.214098\n",
      "Loss at epoch 190: 3.055427\n",
      "Loss at epoch 191: 2.904858\n",
      "Loss at epoch 192: 2.761831\n",
      "Loss at epoch 193: 2.626065\n",
      "Loss at epoch 194: 2.497047\n",
      "Loss at epoch 195: 2.374501\n",
      "Loss at epoch 196: 2.258388\n",
      "Loss at epoch 197: 2.147955\n",
      "Loss at epoch 198: 2.043090\n",
      "Loss at epoch 199: 1.943435\n",
      "Loss at epoch 200: 1.848837\n",
      "Loss at epoch 201: 1.758907\n",
      "Loss at epoch 202: 1.673409\n",
      "Loss at epoch 203: 1.592203\n",
      "Loss at epoch 204: 1.515082\n",
      "Loss at epoch 205: 1.441966\n",
      "Loss at epoch 206: 1.372313\n",
      "Loss at epoch 207: 1.305902\n",
      "Loss at epoch 208: 1.242925\n",
      "Loss at epoch 209: 1.183201\n",
      "Loss at epoch 210: 1.126225\n",
      "Loss at epoch 211: 1.072067\n",
      "Loss at epoch 212: 1.020731\n",
      "Loss at epoch 213: 0.971791\n",
      "Loss at epoch 214: 0.925168\n",
      "Loss at epoch 215: 0.880917\n",
      "Loss at epoch 216: 0.838849\n",
      "Loss at epoch 217: 0.798891\n",
      "Loss at epoch 218: 0.760845\n",
      "Loss at epoch 219: 0.724634\n",
      "Loss at epoch 220: 0.690237\n",
      "Loss at epoch 221: 0.657490\n",
      "Loss at epoch 222: 0.626224\n",
      "Loss at epoch 223: 0.596493\n",
      "Loss at epoch 224: 0.568257\n",
      "Loss at epoch 225: 0.541452\n",
      "Loss at epoch 226: 0.515795\n",
      "Loss at epoch 227: 0.491482\n",
      "Loss at epoch 228: 0.468388\n",
      "Loss at epoch 229: 0.446288\n",
      "Loss at epoch 230: 0.425300\n",
      "Loss at epoch 231: 0.405329\n",
      "Loss at epoch 232: 0.386274\n",
      "Loss at epoch 233: 0.368169\n",
      "Loss at epoch 234: 0.350877\n",
      "Loss at epoch 235: 0.334421\n",
      "Loss at epoch 236: 0.318828\n",
      "Loss at epoch 237: 0.303921\n",
      "Loss at epoch 238: 0.289724\n",
      "Loss at epoch 239: 0.276226\n",
      "Loss at epoch 240: 0.263333\n",
      "Loss at epoch 241: 0.251073\n",
      "Loss at epoch 242: 0.239393\n",
      "Loss at epoch 243: 0.228238\n",
      "Loss at epoch 244: 0.217644\n",
      "Loss at epoch 245: 0.207561\n",
      "Loss at epoch 246: 0.197978\n",
      "Loss at epoch 247: 0.188788\n",
      "Loss at epoch 248: 0.180048\n",
      "Loss at epoch 249: 0.171733\n",
      "Loss at epoch 250: 0.163781\n",
      "Loss at epoch 251: 0.156222\n",
      "Loss at epoch 252: 0.149014\n",
      "Loss at epoch 253: 0.142142\n",
      "Loss at epoch 254: 0.135605\n",
      "Loss at epoch 255: 0.129343\n",
      "Loss at epoch 256: 0.123381\n",
      "Loss at epoch 257: 0.117738\n",
      "Loss at epoch 258: 0.112336\n",
      "Loss at epoch 259: 0.107166\n",
      "Loss at epoch 260: 0.102265\n",
      "Loss at epoch 261: 0.097574\n",
      "Loss at epoch 262: 0.093123\n",
      "Loss at epoch 263: 0.088879\n",
      "Loss at epoch 264: 0.084809\n",
      "Loss at epoch 265: 0.080927\n",
      "Loss at epoch 266: 0.077228\n",
      "Loss at epoch 267: 0.073711\n",
      "Loss at epoch 268: 0.070360\n",
      "Loss at epoch 269: 0.067169\n",
      "Loss at epoch 270: 0.064085\n",
      "Loss at epoch 271: 0.061167\n",
      "Loss at epoch 272: 0.058393\n",
      "Loss at epoch 273: 0.055745\n",
      "Loss at epoch 274: 0.053229\n",
      "Loss at epoch 275: 0.050813\n",
      "Loss at epoch 276: 0.048508\n",
      "Loss at epoch 277: 0.046308\n",
      "Loss at epoch 278: 0.044221\n",
      "Loss at epoch 279: 0.042202\n",
      "Loss at epoch 280: 0.040301\n",
      "Loss at epoch 281: 0.038487\n",
      "Loss at epoch 282: 0.036755\n",
      "Loss at epoch 283: 0.035092\n",
      "Loss at epoch 284: 0.033514\n",
      "Loss at epoch 285: 0.032017\n",
      "Loss at epoch 286: 0.030573\n",
      "Loss at epoch 287: 0.029192\n",
      "Loss at epoch 288: 0.027895\n",
      "Loss at epoch 289: 0.026644\n",
      "Loss at epoch 290: 0.025445\n",
      "Loss at epoch 291: 0.024304\n",
      "Loss at epoch 292: 0.023227\n",
      "Loss at epoch 293: 0.022188\n",
      "Loss at epoch 294: 0.021197\n",
      "Loss at epoch 295: 0.020258\n",
      "Loss at epoch 296: 0.019356\n",
      "Loss at epoch 297: 0.018499\n",
      "Loss at epoch 298: 0.017681\n",
      "Loss at epoch 299: 0.016896\n",
      "Loss at epoch 300: 0.016160\n",
      "Loss at epoch 301: 0.015445\n",
      "Loss at epoch 302: 0.014760\n",
      "Loss at epoch 303: 0.014103\n",
      "Loss at epoch 304: 0.013483\n",
      "Loss at epoch 305: 0.012896\n",
      "Loss at epoch 306: 0.012338\n",
      "Loss at epoch 307: 0.011797\n",
      "Loss at epoch 308: 0.011286\n",
      "Loss at epoch 309: 0.010796\n",
      "Loss at epoch 310: 0.010321\n",
      "Loss at epoch 311: 0.009873\n",
      "Loss at epoch 312: 0.009448\n",
      "Loss at epoch 313: 0.009039\n",
      "Loss at epoch 314: 0.008645\n",
      "Loss at epoch 315: 0.008274\n",
      "Loss at epoch 316: 0.007918\n",
      "Loss at epoch 317: 0.007579\n",
      "Loss at epoch 318: 0.007260\n",
      "Loss at epoch 319: 0.006949\n",
      "Loss at epoch 320: 0.006650\n",
      "Loss at epoch 321: 0.006367\n",
      "Loss at epoch 322: 0.006104\n",
      "Loss at epoch 323: 0.005844\n",
      "Loss at epoch 324: 0.005599\n",
      "Loss at epoch 325: 0.005365\n",
      "Loss at epoch 326: 0.005141\n",
      "Loss at epoch 327: 0.004927\n",
      "Loss at epoch 328: 0.004729\n",
      "Loss at epoch 329: 0.004534\n",
      "Loss at epoch 330: 0.004345\n",
      "Loss at epoch 331: 0.004170\n",
      "Loss at epoch 332: 0.003996\n",
      "Loss at epoch 333: 0.003834\n",
      "Loss at epoch 334: 0.003677\n",
      "Loss at epoch 335: 0.003530\n",
      "Loss at epoch 336: 0.003386\n",
      "Loss at epoch 337: 0.003252\n",
      "Loss at epoch 338: 0.003119\n",
      "Loss at epoch 339: 0.002999\n",
      "Loss at epoch 340: 0.002881\n",
      "Loss at epoch 341: 0.002771\n",
      "Loss at epoch 342: 0.002662\n",
      "Loss at epoch 343: 0.002561\n",
      "Loss at epoch 344: 0.002460\n",
      "Loss at epoch 345: 0.002366\n",
      "Loss at epoch 346: 0.002276\n",
      "Loss at epoch 347: 0.002190\n",
      "Loss at epoch 348: 0.002109\n",
      "Loss at epoch 349: 0.002028\n",
      "Loss at epoch 350: 0.001954\n",
      "Loss at epoch 351: 0.001883\n",
      "Loss at epoch 352: 0.001815\n",
      "Loss at epoch 353: 0.001747\n",
      "Loss at epoch 354: 0.001683\n",
      "Loss at epoch 355: 0.001621\n",
      "Loss at epoch 356: 0.001566\n",
      "Loss at epoch 357: 0.001507\n",
      "Loss at epoch 358: 0.001455\n",
      "Loss at epoch 359: 0.001404\n",
      "Loss at epoch 360: 0.001354\n",
      "Loss at epoch 361: 0.001308\n",
      "Loss at epoch 362: 0.001264\n",
      "Loss at epoch 363: 0.001221\n",
      "Loss at epoch 364: 0.001180\n",
      "Loss at epoch 365: 0.001142\n",
      "Loss at epoch 366: 0.001103\n",
      "Loss at epoch 367: 0.001067\n",
      "Loss at epoch 368: 0.001031\n",
      "Loss at epoch 369: 0.000997\n",
      "Loss at epoch 370: 0.000965\n",
      "Loss at epoch 371: 0.000934\n",
      "Loss at epoch 372: 0.000903\n",
      "Loss at epoch 373: 0.000876\n",
      "Loss at epoch 374: 0.000848\n",
      "Loss at epoch 375: 0.000820\n",
      "Loss at epoch 376: 0.000795\n",
      "Loss at epoch 377: 0.000770\n",
      "Loss at epoch 378: 0.000747\n",
      "Loss at epoch 379: 0.000725\n",
      "Loss at epoch 380: 0.000703\n",
      "Loss at epoch 381: 0.000683\n",
      "Loss at epoch 382: 0.000662\n",
      "Loss at epoch 383: 0.000643\n",
      "Loss at epoch 384: 0.000625\n",
      "Loss at epoch 385: 0.000607\n",
      "Loss at epoch 386: 0.000589\n",
      "Loss at epoch 387: 0.000573\n",
      "Loss at epoch 388: 0.000557\n",
      "Loss at epoch 389: 0.000541\n",
      "Loss at epoch 390: 0.000526\n",
      "Loss at epoch 391: 0.000511\n",
      "Loss at epoch 392: 0.000498\n",
      "Loss at epoch 393: 0.000483\n",
      "Loss at epoch 394: 0.000470\n",
      "Loss at epoch 395: 0.000457\n",
      "Loss at epoch 396: 0.000446\n",
      "Loss at epoch 397: 0.000434\n",
      "Loss at epoch 398: 0.000422\n",
      "Loss at epoch 399: 0.000412\n",
      "Loss at epoch 400: 0.000401\n",
      "Loss at epoch 401: 0.000390\n",
      "Loss at epoch 402: 0.000381\n",
      "Loss at epoch 403: 0.000371\n",
      "Loss at epoch 404: 0.000361\n",
      "Loss at epoch 405: 0.000353\n",
      "Loss at epoch 406: 0.000345\n",
      "Loss at epoch 407: 0.000336\n",
      "Loss at epoch 408: 0.000327\n",
      "Loss at epoch 409: 0.000319\n",
      "Loss at epoch 410: 0.000311\n",
      "Loss at epoch 411: 0.000304\n",
      "Loss at epoch 412: 0.000297\n",
      "Loss at epoch 413: 0.000290\n",
      "Loss at epoch 414: 0.000284\n",
      "Loss at epoch 415: 0.000277\n",
      "Loss at epoch 416: 0.000271\n",
      "Loss at epoch 417: 0.000265\n",
      "Loss at epoch 418: 0.000259\n",
      "Loss at epoch 419: 0.000254\n",
      "Loss at epoch 420: 0.000248\n",
      "Loss at epoch 421: 0.000243\n",
      "Loss at epoch 422: 0.000237\n",
      "Loss at epoch 423: 0.000232\n",
      "Loss at epoch 424: 0.000227\n",
      "Loss at epoch 425: 0.000222\n",
      "Loss at epoch 426: 0.000218\n",
      "Loss at epoch 427: 0.000213\n",
      "Loss at epoch 428: 0.000209\n",
      "Loss at epoch 429: 0.000205\n",
      "Loss at epoch 430: 0.000201\n",
      "Loss at epoch 431: 0.000197\n",
      "Loss at epoch 432: 0.000193\n",
      "Loss at epoch 433: 0.000189\n",
      "Loss at epoch 434: 0.000185\n",
      "Loss at epoch 435: 0.000181\n",
      "Loss at epoch 436: 0.000178\n",
      "Loss at epoch 437: 0.000175\n",
      "Loss at epoch 438: 0.000172\n",
      "Loss at epoch 439: 0.000168\n",
      "Loss at epoch 440: 0.000165\n",
      "Loss at epoch 441: 0.000161\n",
      "Loss at epoch 442: 0.000158\n",
      "Loss at epoch 443: 0.000155\n",
      "Loss at epoch 444: 0.000153\n",
      "Loss at epoch 445: 0.000150\n",
      "Loss at epoch 446: 0.000147\n",
      "Loss at epoch 447: 0.000144\n",
      "Loss at epoch 448: 0.000142\n",
      "Loss at epoch 449: 0.000139\n",
      "Loss at epoch 450: 0.000136\n",
      "Loss at epoch 451: 0.000134\n",
      "Loss at epoch 452: 0.000132\n",
      "Loss at epoch 453: 0.000129\n",
      "Loss at epoch 454: 0.000127\n",
      "Loss at epoch 455: 0.000125\n",
      "Loss at epoch 456: 0.000123\n",
      "Loss at epoch 457: 0.000121\n",
      "Loss at epoch 458: 0.000119\n",
      "Loss at epoch 459: 0.000117\n",
      "Loss at epoch 460: 0.000116\n",
      "Loss at epoch 461: 0.000114\n",
      "Loss at epoch 462: 0.000112\n",
      "Loss at epoch 463: 0.000110\n",
      "Loss at epoch 464: 0.000108\n",
      "Loss at epoch 465: 0.000106\n",
      "Loss at epoch 466: 0.000105\n",
      "Loss at epoch 467: 0.000103\n",
      "Loss at epoch 468: 0.000101\n",
      "Loss at epoch 469: 0.000100\n",
      "Loss at epoch 470: 0.000098\n",
      "Loss at epoch 471: 0.000097\n",
      "Loss at epoch 472: 0.000095\n",
      "Loss at epoch 473: 0.000094\n",
      "Loss at epoch 474: 0.000093\n",
      "Loss at epoch 475: 0.000091\n",
      "Loss at epoch 476: 0.000090\n",
      "Loss at epoch 477: 0.000088\n",
      "Loss at epoch 478: 0.000087\n",
      "Loss at epoch 479: 0.000086\n",
      "Loss at epoch 480: 0.000085\n",
      "Loss at epoch 481: 0.000084\n",
      "Loss at epoch 482: 0.000083\n",
      "Loss at epoch 483: 0.000081\n",
      "Loss at epoch 484: 0.000080\n",
      "Loss at epoch 485: 0.000079\n",
      "Loss at epoch 486: 0.000078\n",
      "Loss at epoch 487: 0.000077\n",
      "Loss at epoch 488: 0.000076\n",
      "Loss at epoch 489: 0.000075\n",
      "Loss at epoch 490: 0.000074\n",
      "Loss at epoch 491: 0.000072\n",
      "Loss at epoch 492: 0.000072\n",
      "Loss at epoch 493: 0.000071\n",
      "Loss at epoch 494: 0.000070\n",
      "Loss at epoch 495: 0.000069\n",
      "Loss at epoch 496: 0.000068\n",
      "Loss at epoch 497: 0.000067\n",
      "Loss at epoch 498: 0.000067\n",
      "Loss at epoch 499: 0.000065\n"
     ]
    }
   ],
   "source": [
    "#Simple forward and backwards pass\n",
    "batch_size, D_in, hidden_size, D_out = 64, 1000, 100, 10\n",
    "lr = 1e-6\n",
    "dtype = torch.FloatTensor #CPU\n",
    "dtype = torch.cuda.FloatTensor #GPU\n",
    "\n",
    "x = Variable(torch.randn(batch_size, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(batch_size, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, hidden_size).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(hidden_size, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "for epoch in range(500):\n",
    "    #Clamp @ min=0 is equivalent to relu\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    #L2 loss - Loss is Variable shape (1,), so loss.data is Tensor of shape (1,)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    print(\"Loss at epoch %d: %f\" % (epoch, loss.data[0]))\n",
    "    \n",
    "    #Autograd. Computes backward pass wrt all Variables with requires_grad=True\n",
    "    #After this op, all Variables with requires_grad will hold the gradient of the loss with w1 and w2\n",
    "    #NOTE: PyTorch ADDS to the current grad.data in Variables in backward(), so remember to reset\n",
    "    loss.backward()\n",
    "    \n",
    "    #SGD\n",
    "    w1.data -= lr * w1.grad.data\n",
    "    w2.data -= lr * w2.grad.data\n",
    "    \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining new autograd functions\n",
    "Each autograd operator is just a class with two functions: forward() and backward()\n",
    "- `forward()` : Compute output Tensors from input Tensors\n",
    "- `backward()`: Compute gradient of input Tensors with respect to that some scalar value. Receives the gradient of the output Tensor wrt to some scalar value (loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 35289344.000000\n",
      "Loss at epoch 1: 35238464.000000\n",
      "Loss at epoch 2: 34307684.000000\n",
      "Loss at epoch 3: 28130868.000000\n",
      "Loss at epoch 4: 18151094.000000\n",
      "Loss at epoch 5: 9870788.000000\n",
      "Loss at epoch 6: 5244858.500000\n",
      "Loss at epoch 7: 3235786.250000\n",
      "Loss at epoch 8: 2369459.000000\n",
      "Loss at epoch 9: 1927296.875000\n",
      "Loss at epoch 10: 1642875.500000\n",
      "Loss at epoch 11: 1428844.375000\n",
      "Loss at epoch 12: 1256006.500000\n",
      "Loss at epoch 13: 1112217.875000\n",
      "Loss at epoch 14: 990908.312500\n",
      "Loss at epoch 15: 887374.187500\n",
      "Loss at epoch 16: 798356.250000\n",
      "Loss at epoch 17: 721344.375000\n",
      "Loss at epoch 18: 654369.750000\n",
      "Loss at epoch 19: 595734.125000\n",
      "Loss at epoch 20: 544118.062500\n",
      "Loss at epoch 21: 498466.312500\n",
      "Loss at epoch 22: 457903.031250\n",
      "Loss at epoch 23: 421703.468750\n",
      "Loss at epoch 24: 389290.250000\n",
      "Loss at epoch 25: 360161.562500\n",
      "Loss at epoch 26: 333908.312500\n",
      "Loss at epoch 27: 310173.500000\n",
      "Loss at epoch 28: 288620.875000\n",
      "Loss at epoch 29: 269008.437500\n",
      "Loss at epoch 30: 251126.078125\n",
      "Loss at epoch 31: 234782.968750\n",
      "Loss at epoch 32: 219813.890625\n",
      "Loss at epoch 33: 206080.187500\n",
      "Loss at epoch 34: 193457.625000\n",
      "Loss at epoch 35: 181825.156250\n",
      "Loss at epoch 36: 171121.890625\n",
      "Loss at epoch 37: 161226.406250\n",
      "Loss at epoch 38: 152058.656250\n",
      "Loss at epoch 39: 143552.828125\n",
      "Loss at epoch 40: 135650.000000\n",
      "Loss at epoch 41: 128295.304688\n",
      "Loss at epoch 42: 121446.015625\n",
      "Loss at epoch 43: 115060.406250\n",
      "Loss at epoch 44: 109094.218750\n",
      "Loss at epoch 45: 103520.710938\n",
      "Loss at epoch 46: 98302.609375\n",
      "Loss at epoch 47: 93413.375000\n",
      "Loss at epoch 48: 88826.265625\n",
      "Loss at epoch 49: 84516.093750\n",
      "Loss at epoch 50: 80464.601562\n",
      "Loss at epoch 51: 76653.437500\n",
      "Loss at epoch 52: 73067.671875\n",
      "Loss at epoch 53: 69687.773438\n",
      "Loss at epoch 54: 66498.914062\n",
      "Loss at epoch 55: 63488.550781\n",
      "Loss at epoch 56: 60647.746094\n",
      "Loss at epoch 57: 57962.640625\n",
      "Loss at epoch 58: 55423.292969\n",
      "Loss at epoch 59: 53019.316406\n",
      "Loss at epoch 60: 50741.378906\n",
      "Loss at epoch 61: 48583.804688\n",
      "Loss at epoch 62: 46539.445312\n",
      "Loss at epoch 63: 44599.289062\n",
      "Loss at epoch 64: 42759.656250\n",
      "Loss at epoch 65: 41011.750000\n",
      "Loss at epoch 66: 39349.648438\n",
      "Loss at epoch 67: 37768.253906\n",
      "Loss at epoch 68: 36263.980469\n",
      "Loss at epoch 69: 34832.289062\n",
      "Loss at epoch 70: 33467.980469\n",
      "Loss at epoch 71: 32168.302734\n",
      "Loss at epoch 72: 30930.955078\n",
      "Loss at epoch 73: 29750.726562\n",
      "Loss at epoch 74: 28623.818359\n",
      "Loss at epoch 75: 27547.326172\n",
      "Loss at epoch 76: 26518.746094\n",
      "Loss at epoch 77: 25537.886719\n",
      "Loss at epoch 78: 24600.294922\n",
      "Loss at epoch 79: 23703.644531\n",
      "Loss at epoch 80: 22845.539062\n",
      "Loss at epoch 81: 22024.853516\n",
      "Loss at epoch 82: 21239.281250\n",
      "Loss at epoch 83: 20486.501953\n",
      "Loss at epoch 84: 19764.921875\n",
      "Loss at epoch 85: 19073.306641\n",
      "Loss at epoch 86: 18410.320312\n",
      "Loss at epoch 87: 17774.775391\n",
      "Loss at epoch 88: 17164.750000\n",
      "Loss at epoch 89: 16579.027344\n",
      "Loss at epoch 90: 16016.489258\n",
      "Loss at epoch 91: 15475.999023\n",
      "Loss at epoch 92: 14956.448242\n",
      "Loss at epoch 93: 14456.947266\n",
      "Loss at epoch 94: 13977.484375\n",
      "Loss at epoch 95: 13516.841797\n",
      "Loss at epoch 96: 13073.810547\n",
      "Loss at epoch 97: 12647.669922\n",
      "Loss at epoch 98: 12237.478516\n",
      "Loss at epoch 99: 11842.626953\n",
      "Loss at epoch 100: 11462.464844\n",
      "Loss at epoch 101: 11096.381836\n",
      "Loss at epoch 102: 10743.911133\n",
      "Loss at epoch 103: 10404.287109\n",
      "Loss at epoch 104: 10077.087891\n",
      "Loss at epoch 105: 9761.950195\n",
      "Loss at epoch 106: 9458.095703\n",
      "Loss at epoch 107: 9165.042969\n",
      "Loss at epoch 108: 8882.351562\n",
      "Loss at epoch 109: 8609.607422\n",
      "Loss at epoch 110: 8346.405273\n",
      "Loss at epoch 111: 8092.361816\n",
      "Loss at epoch 112: 7847.119141\n",
      "Loss at epoch 113: 7610.326172\n",
      "Loss at epoch 114: 7381.731445\n",
      "Loss at epoch 115: 7161.628906\n",
      "Loss at epoch 116: 6949.114258\n",
      "Loss at epoch 117: 6743.778320\n",
      "Loss at epoch 118: 6545.342285\n",
      "Loss at epoch 119: 6353.541992\n",
      "Loss at epoch 120: 6168.251953\n",
      "Loss at epoch 121: 5989.125977\n",
      "Loss at epoch 122: 5815.903809\n",
      "Loss at epoch 123: 5648.346680\n",
      "Loss at epoch 124: 5486.253906\n",
      "Loss at epoch 125: 5329.424316\n",
      "Loss at epoch 126: 5177.653809\n",
      "Loss at epoch 127: 5030.774902\n",
      "Loss at epoch 128: 4888.591309\n",
      "Loss at epoch 129: 4750.924805\n",
      "Loss at epoch 130: 4617.864746\n",
      "Loss at epoch 131: 4489.025879\n",
      "Loss at epoch 132: 4364.279785\n",
      "Loss at epoch 133: 4243.432129\n",
      "Loss at epoch 134: 4126.349609\n",
      "Loss at epoch 135: 4012.904053\n",
      "Loss at epoch 136: 3902.954590\n",
      "Loss at epoch 137: 3796.391113\n",
      "Loss at epoch 138: 3693.090332\n",
      "Loss at epoch 139: 3592.936279\n",
      "Loss at epoch 140: 3495.826172\n",
      "Loss at epoch 141: 3401.650391\n",
      "Loss at epoch 142: 3310.317627\n",
      "Loss at epoch 143: 3221.726318\n",
      "Loss at epoch 144: 3135.775879\n",
      "Loss at epoch 145: 3052.388672\n",
      "Loss at epoch 146: 2971.477539\n",
      "Loss at epoch 147: 2892.951660\n",
      "Loss at epoch 148: 2816.745605\n",
      "Loss at epoch 149: 2742.772461\n",
      "Loss at epoch 150: 2670.958252\n",
      "Loss at epoch 151: 2601.237793\n",
      "Loss at epoch 152: 2533.539795\n",
      "Loss at epoch 153: 2467.841553\n",
      "Loss at epoch 154: 2404.070312\n",
      "Loss at epoch 155: 2342.130859\n",
      "Loss at epoch 156: 2281.958984\n",
      "Loss at epoch 157: 2223.505615\n",
      "Loss at epoch 158: 2166.709717\n",
      "Loss at epoch 159: 2111.528320\n",
      "Loss at epoch 160: 2057.900146\n",
      "Loss at epoch 161: 2005.781860\n",
      "Loss at epoch 162: 1955.121338\n",
      "Loss at epoch 163: 1905.877686\n",
      "Loss at epoch 164: 1858.006836\n",
      "Loss at epoch 165: 1811.460205\n",
      "Loss at epoch 166: 1766.207764\n",
      "Loss at epoch 167: 1722.223389\n",
      "Loss at epoch 168: 1679.463379\n",
      "Loss at epoch 169: 1637.873047\n",
      "Loss at epoch 170: 1597.419067\n",
      "Loss at epoch 171: 1558.062988\n",
      "Loss at epoch 172: 1519.781006\n",
      "Loss at epoch 173: 1482.532837\n",
      "Loss at epoch 174: 1446.290161\n",
      "Loss at epoch 175: 1411.020508\n",
      "Loss at epoch 176: 1376.704346\n",
      "Loss at epoch 177: 1343.297485\n",
      "Loss at epoch 178: 1310.791504\n",
      "Loss at epoch 179: 1279.142944\n",
      "Loss at epoch 180: 1248.332520\n",
      "Loss at epoch 181: 1218.341553\n",
      "Loss at epoch 182: 1189.139038\n",
      "Loss at epoch 183: 1160.707153\n",
      "Loss at epoch 184: 1133.017334\n",
      "Loss at epoch 185: 1106.053833\n",
      "Loss at epoch 186: 1079.790527\n",
      "Loss at epoch 187: 1054.204224\n",
      "Loss at epoch 188: 1029.281616\n",
      "Loss at epoch 189: 1005.003479\n",
      "Loss at epoch 190: 981.354492\n",
      "Loss at epoch 191: 958.312622\n",
      "Loss at epoch 192: 935.861633\n",
      "Loss at epoch 193: 913.987976\n",
      "Loss at epoch 194: 892.671997\n",
      "Loss at epoch 195: 871.897583\n",
      "Loss at epoch 196: 851.654968\n",
      "Loss at epoch 197: 831.921387\n",
      "Loss at epoch 198: 812.685608\n",
      "Loss at epoch 199: 793.944092\n",
      "Loss at epoch 200: 775.667542\n",
      "Loss at epoch 201: 757.847107\n",
      "Loss at epoch 202: 740.477722\n",
      "Loss at epoch 203: 723.545227\n",
      "Loss at epoch 204: 707.042725\n",
      "Loss at epoch 205: 690.949036\n",
      "Loss at epoch 206: 675.260315\n",
      "Loss at epoch 207: 659.965210\n",
      "Loss at epoch 208: 645.048889\n",
      "Loss at epoch 209: 630.500610\n",
      "Loss at epoch 210: 616.309998\n",
      "Loss at epoch 211: 602.464539\n",
      "Loss at epoch 212: 588.959045\n",
      "Loss at epoch 213: 575.781738\n",
      "Loss at epoch 214: 562.922058\n",
      "Loss at epoch 215: 550.377686\n",
      "Loss at epoch 216: 538.132019\n",
      "Loss at epoch 217: 526.184021\n",
      "Loss at epoch 218: 514.525085\n",
      "Loss at epoch 219: 503.144501\n",
      "Loss at epoch 220: 492.035583\n",
      "Loss at epoch 221: 481.195038\n",
      "Loss at epoch 222: 470.611816\n",
      "Loss at epoch 223: 460.280975\n",
      "Loss at epoch 224: 450.194427\n",
      "Loss at epoch 225: 440.344299\n",
      "Loss at epoch 226: 430.729736\n",
      "Loss at epoch 227: 421.342957\n",
      "Loss at epoch 228: 412.179108\n",
      "Loss at epoch 229: 403.231445\n",
      "Loss at epoch 230: 394.492767\n",
      "Loss at epoch 231: 385.957275\n",
      "Loss at epoch 232: 377.623993\n",
      "Loss at epoch 233: 369.483002\n",
      "Loss at epoch 234: 361.530884\n",
      "Loss at epoch 235: 353.765259\n",
      "Loss at epoch 236: 346.176514\n",
      "Loss at epoch 237: 338.765045\n",
      "Loss at epoch 238: 331.523621\n",
      "Loss at epoch 239: 324.447784\n",
      "Loss at epoch 240: 317.534241\n",
      "Loss at epoch 241: 310.780884\n",
      "Loss at epoch 242: 304.181458\n",
      "Loss at epoch 243: 297.731476\n",
      "Loss at epoch 244: 291.429901\n",
      "Loss at epoch 245: 285.270599\n",
      "Loss at epoch 246: 279.251526\n",
      "Loss at epoch 247: 273.367096\n",
      "Loss at epoch 248: 267.616669\n",
      "Loss at epoch 249: 261.996399\n",
      "Loss at epoch 250: 256.502045\n",
      "Loss at epoch 251: 251.133301\n",
      "Loss at epoch 252: 245.883896\n",
      "Loss at epoch 253: 240.753204\n",
      "Loss at epoch 254: 235.734207\n",
      "Loss at epoch 255: 230.828766\n",
      "Loss at epoch 256: 226.032898\n",
      "Loss at epoch 257: 221.343170\n",
      "Loss at epoch 258: 216.756821\n",
      "Loss at epoch 259: 212.272247\n",
      "Loss at epoch 260: 207.887497\n",
      "Loss at epoch 261: 203.598099\n",
      "Loss at epoch 262: 199.403412\n",
      "Loss at epoch 263: 195.301331\n",
      "Loss at epoch 264: 191.288605\n",
      "Loss at epoch 265: 187.363556\n",
      "Loss at epoch 266: 183.525497\n",
      "Loss at epoch 267: 179.770401\n",
      "Loss at epoch 268: 176.096237\n",
      "Loss at epoch 269: 172.502213\n",
      "Loss at epoch 270: 168.986649\n",
      "Loss at epoch 271: 165.547241\n",
      "Loss at epoch 272: 162.183014\n",
      "Loss at epoch 273: 158.888916\n",
      "Loss at epoch 274: 155.667831\n",
      "Loss at epoch 275: 152.515106\n",
      "Loss at epoch 276: 149.430344\n",
      "Loss at epoch 277: 146.411438\n",
      "Loss at epoch 278: 143.458801\n",
      "Loss at epoch 279: 140.567230\n",
      "Loss at epoch 280: 137.737778\n",
      "Loss at epoch 281: 134.969330\n",
      "Loss at epoch 282: 132.258530\n",
      "Loss at epoch 283: 129.606201\n",
      "Loss at epoch 284: 127.011086\n",
      "Loss at epoch 285: 124.470001\n",
      "Loss at epoch 286: 121.983383\n",
      "Loss at epoch 287: 119.548592\n",
      "Loss at epoch 288: 117.164978\n",
      "Loss at epoch 289: 114.835052\n",
      "Loss at epoch 290: 112.556671\n",
      "Loss at epoch 291: 110.326416\n",
      "Loss at epoch 292: 108.143837\n",
      "Loss at epoch 293: 106.006264\n",
      "Loss at epoch 294: 103.912712\n",
      "Loss at epoch 295: 101.863220\n",
      "Loss at epoch 296: 99.856522\n",
      "Loss at epoch 297: 97.890686\n",
      "Loss at epoch 298: 95.966187\n",
      "Loss at epoch 299: 94.081421\n",
      "Loss at epoch 300: 92.235550\n",
      "Loss at epoch 301: 90.428268\n",
      "Loss at epoch 302: 88.657387\n",
      "Loss at epoch 303: 86.924492\n",
      "Loss at epoch 304: 85.226204\n",
      "Loss at epoch 305: 83.562454\n",
      "Loss at epoch 306: 81.933685\n",
      "Loss at epoch 307: 80.338181\n",
      "Loss at epoch 308: 78.773911\n",
      "Loss at epoch 309: 77.241982\n",
      "Loss at epoch 310: 75.742989\n",
      "Loss at epoch 311: 74.273445\n",
      "Loss at epoch 312: 72.833435\n",
      "Loss at epoch 313: 71.422783\n",
      "Loss at epoch 314: 70.040710\n",
      "Loss at epoch 315: 68.687126\n",
      "Loss at epoch 316: 67.360397\n",
      "Loss at epoch 317: 66.061119\n",
      "Loss at epoch 318: 64.787247\n",
      "Loss at epoch 319: 63.539310\n",
      "Loss at epoch 320: 62.316689\n",
      "Loss at epoch 321: 61.118324\n",
      "Loss at epoch 322: 59.944229\n",
      "Loss at epoch 323: 58.793678\n",
      "Loss at epoch 324: 57.666721\n",
      "Loss at epoch 325: 56.562576\n",
      "Loss at epoch 326: 55.479027\n",
      "Loss at epoch 327: 54.418415\n",
      "Loss at epoch 328: 53.377975\n",
      "Loss at epoch 329: 52.358921\n",
      "Loss at epoch 330: 51.359425\n",
      "Loss at epoch 331: 50.380333\n",
      "Loss at epoch 332: 49.420433\n",
      "Loss at epoch 333: 48.479858\n",
      "Loss at epoch 334: 47.557827\n",
      "Loss at epoch 335: 46.654034\n",
      "Loss at epoch 336: 45.767944\n",
      "Loss at epoch 337: 44.899971\n",
      "Loss at epoch 338: 44.048126\n",
      "Loss at epoch 339: 43.213615\n",
      "Loss at epoch 340: 42.395363\n",
      "Loss at epoch 341: 41.593666\n",
      "Loss at epoch 342: 40.807186\n",
      "Loss at epoch 343: 40.036522\n",
      "Loss at epoch 344: 39.280598\n",
      "Loss at epoch 345: 38.539425\n",
      "Loss at epoch 346: 37.813564\n",
      "Loss at epoch 347: 37.101078\n",
      "Loss at epoch 348: 36.402637\n",
      "Loss at epoch 349: 35.717766\n",
      "Loss at epoch 350: 35.046806\n",
      "Loss at epoch 351: 34.388618\n",
      "Loss at epoch 352: 33.742760\n",
      "Loss at epoch 353: 33.110359\n",
      "Loss at epoch 354: 32.489456\n",
      "Loss at epoch 355: 31.880796\n",
      "Loss at epoch 356: 31.284559\n",
      "Loss at epoch 357: 30.699266\n",
      "Loss at epoch 358: 30.125338\n",
      "Loss at epoch 359: 29.562435\n",
      "Loss at epoch 360: 29.010601\n",
      "Loss at epoch 361: 28.468876\n",
      "Loss at epoch 362: 27.938990\n",
      "Loss at epoch 363: 27.417944\n",
      "Loss at epoch 364: 26.907084\n",
      "Loss at epoch 365: 26.406654\n",
      "Loss at epoch 366: 25.915506\n",
      "Loss at epoch 367: 25.433752\n",
      "Loss at epoch 368: 24.961903\n",
      "Loss at epoch 369: 24.498253\n",
      "Loss at epoch 370: 24.043970\n",
      "Loss at epoch 371: 23.597895\n",
      "Loss at epoch 372: 23.160744\n",
      "Loss at epoch 373: 22.732058\n",
      "Loss at epoch 374: 22.311647\n",
      "Loss at epoch 375: 21.899050\n",
      "Loss at epoch 376: 21.494732\n",
      "Loss at epoch 377: 21.097490\n",
      "Loss at epoch 378: 20.708799\n",
      "Loss at epoch 379: 20.326855\n",
      "Loss at epoch 380: 19.951683\n",
      "Loss at epoch 381: 19.584644\n",
      "Loss at epoch 382: 19.224342\n",
      "Loss at epoch 383: 18.870310\n",
      "Loss at epoch 384: 18.523563\n",
      "Loss at epoch 385: 18.183369\n",
      "Loss at epoch 386: 17.849106\n",
      "Loss at epoch 387: 17.521660\n",
      "Loss at epoch 388: 17.200092\n",
      "Loss at epoch 389: 16.884748\n",
      "Loss at epoch 390: 16.575342\n",
      "Loss at epoch 391: 16.271881\n",
      "Loss at epoch 392: 15.974104\n",
      "Loss at epoch 393: 15.681699\n",
      "Loss at epoch 394: 15.395056\n",
      "Loss at epoch 395: 15.113729\n",
      "Loss at epoch 396: 14.837822\n",
      "Loss at epoch 397: 14.566504\n",
      "Loss at epoch 398: 14.301128\n",
      "Loss at epoch 399: 14.040084\n",
      "Loss at epoch 400: 13.784504\n",
      "Loss at epoch 401: 13.533380\n",
      "Loss at epoch 402: 13.286830\n",
      "Loss at epoch 403: 13.045241\n",
      "Loss at epoch 404: 12.807815\n",
      "Loss at epoch 405: 12.574891\n",
      "Loss at epoch 406: 12.346361\n",
      "Loss at epoch 407: 12.122238\n",
      "Loss at epoch 408: 11.902195\n",
      "Loss at epoch 409: 11.686230\n",
      "Loss at epoch 410: 11.474074\n",
      "Loss at epoch 411: 11.266135\n",
      "Loss at epoch 412: 11.062165\n",
      "Loss at epoch 413: 10.861859\n",
      "Loss at epoch 414: 10.665163\n",
      "Loss at epoch 415: 10.472315\n",
      "Loss at epoch 416: 10.283139\n",
      "Loss at epoch 417: 10.097218\n",
      "Loss at epoch 418: 9.914832\n",
      "Loss at epoch 419: 9.735643\n",
      "Loss at epoch 420: 9.560054\n",
      "Loss at epoch 421: 9.387453\n",
      "Loss at epoch 422: 9.218381\n",
      "Loss at epoch 423: 9.052186\n",
      "Loss at epoch 424: 8.889230\n",
      "Loss at epoch 425: 8.728708\n",
      "Loss at epoch 426: 8.571777\n",
      "Loss at epoch 427: 8.417571\n",
      "Loss at epoch 428: 8.266129\n",
      "Loss at epoch 429: 8.117569\n",
      "Loss at epoch 430: 7.971480\n",
      "Loss at epoch 431: 7.828583\n",
      "Loss at epoch 432: 7.687988\n",
      "Loss at epoch 433: 7.550049\n",
      "Loss at epoch 434: 7.414343\n",
      "Loss at epoch 435: 7.281564\n",
      "Loss at epoch 436: 7.150918\n",
      "Loss at epoch 437: 7.023048\n",
      "Loss at epoch 438: 6.897204\n",
      "Loss at epoch 439: 6.773753\n",
      "Loss at epoch 440: 6.652504\n",
      "Loss at epoch 441: 6.533439\n",
      "Loss at epoch 442: 6.416748\n",
      "Loss at epoch 443: 6.301992\n",
      "Loss at epoch 444: 6.189521\n",
      "Loss at epoch 445: 6.079060\n",
      "Loss at epoch 446: 5.970517\n",
      "Loss at epoch 447: 5.863940\n",
      "Loss at epoch 448: 5.759244\n",
      "Loss at epoch 449: 5.656474\n",
      "Loss at epoch 450: 5.555929\n",
      "Loss at epoch 451: 5.456919\n",
      "Loss at epoch 452: 5.359740\n",
      "Loss at epoch 453: 5.264278\n",
      "Loss at epoch 454: 5.170626\n",
      "Loss at epoch 455: 5.078674\n",
      "Loss at epoch 456: 4.988289\n",
      "Loss at epoch 457: 4.899682\n",
      "Loss at epoch 458: 4.812705\n",
      "Loss at epoch 459: 4.727182\n",
      "Loss at epoch 460: 4.643230\n",
      "Loss at epoch 461: 4.560783\n",
      "Loss at epoch 462: 4.479925\n",
      "Loss at epoch 463: 4.400471\n",
      "Loss at epoch 464: 4.322411\n",
      "Loss at epoch 465: 4.245739\n",
      "Loss at epoch 466: 4.170691\n",
      "Loss at epoch 467: 4.096908\n",
      "Loss at epoch 468: 4.024434\n",
      "Loss at epoch 469: 3.953008\n",
      "Loss at epoch 470: 3.883114\n",
      "Loss at epoch 471: 3.814492\n",
      "Loss at epoch 472: 3.747098\n",
      "Loss at epoch 473: 3.680735\n",
      "Loss at epoch 474: 3.615795\n",
      "Loss at epoch 475: 3.552089\n",
      "Loss at epoch 476: 3.489078\n",
      "Loss at epoch 477: 3.427581\n",
      "Loss at epoch 478: 3.367237\n",
      "Loss at epoch 479: 3.307781\n",
      "Loss at epoch 480: 3.249421\n",
      "Loss at epoch 481: 3.192218\n",
      "Loss at epoch 482: 3.135911\n",
      "Loss at epoch 483: 3.080768\n",
      "Loss at epoch 484: 3.026526\n",
      "Loss at epoch 485: 2.973360\n",
      "Loss at epoch 486: 2.920923\n",
      "Loss at epoch 487: 2.869644\n",
      "Loss at epoch 488: 2.819165\n",
      "Loss at epoch 489: 2.769684\n",
      "Loss at epoch 490: 2.721040\n",
      "Loss at epoch 491: 2.673283\n",
      "Loss at epoch 492: 2.626225\n",
      "Loss at epoch 493: 2.580174\n",
      "Loss at epoch 494: 2.534860\n",
      "Loss at epoch 495: 2.490458\n",
      "Loss at epoch 496: 2.446860\n",
      "Loss at epoch 497: 2.403936\n",
      "Loss at epoch 498: 2.361709\n",
      "Loss at epoch 499: 2.320598\n"
     ]
    }
   ],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Subclass torch.autograd.Function, then implement forward() and backward()\n",
    "    \"\"\"\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        We can cache any Tensor for use in backward pass by using save_for_backward() method\n",
    "        \"\"\"\n",
    "        self.save_for_backward(input) #NOTE: this only saves input or output, but NOT intermediate\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Input to this will be a Tensor (not variable), containing gradient wrt to output\"\"\"\n",
    "        input, = self.saved_tensors #Outputs in a tuple\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[grad_input < 0] = 0.\n",
    "        return grad_input\n",
    "    \n",
    "dtype = torch.cuda.FloatTensor\n",
    "batch_size, D_in, hidden_size, D_out = 64, 1000, 100, 10\n",
    "lr = 1e-6\n",
    "\n",
    "x = Variable(torch.randn(batch_size, D_in).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.randn(batch_size, D_out).type(dtype), requires_grad=False)\n",
    "\n",
    "w1 = Variable(torch.randn(D_in, hidden_size).type(dtype), requires_grad=True)\n",
    "w2 = Variable(torch.randn(hidden_size, D_out).type(dtype), requires_grad=True)\n",
    "\n",
    "for epoch in range(500):\n",
    "    relu = MyReLU()\n",
    "    #Clamp @ min=0 is equivalent to relu\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    \n",
    "    #L2 loss - Loss is Variable shape (1,), so loss.data is Tensor of shape (1,)\n",
    "    loss = (y_pred-y).pow(2).sum()\n",
    "    print(\"Loss at epoch %d: %f\" % (epoch, loss.data[0]))\n",
    "    \n",
    "    #Autograd. Computes backward pass wrt all Variables with requires_grad=True\n",
    "    #After this op, all Variables with requires_grad will hold the gradient of the loss with w1 and w2\n",
    "    #NOTE: PyTorch ADDS to the current grad.data in Variables in backward(), so remember to reset\n",
    "    loss.backward()\n",
    "    \n",
    "    #SGD\n",
    "    w1.data -= lr * w1.grad.data\n",
    "    w2.data -= lr * w2.grad.data\n",
    "    \n",
    "    w1.grad.data.zero_()\n",
    "    w2.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher level layers - Example with nn module\n",
    "Sometimes, raw autograd can be too low-level. PyTorch offers a higher level abstraction with `nn` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0 662.49462890625\n",
      "1 611.0245361328125\n",
      "2 565.9374389648438\n",
      "3 526.3433227539062\n",
      "4 491.1385803222656\n",
      "5 459.2030029296875\n",
      "6 430.5286865234375\n",
      "7 404.2904052734375\n",
      "8 380.19427490234375\n",
      "9 358.1907653808594\n",
      "10 337.7488098144531\n",
      "11 318.6429748535156\n",
      "12 300.6151428222656\n",
      "13 283.73333740234375\n",
      "14 267.9111328125\n",
      "15 253.01820373535156\n",
      "16 238.95181274414062\n",
      "17 225.61273193359375\n",
      "18 212.92849731445312\n",
      "19 200.87100219726562\n",
      "20 189.3870849609375\n",
      "21 178.47413635253906\n",
      "22 168.12863159179688\n",
      "23 158.35069274902344\n",
      "24 149.1097412109375\n",
      "25 140.37469482421875\n",
      "26 132.1210479736328\n",
      "27 124.33000946044922\n",
      "28 116.9700927734375\n",
      "29 110.01622009277344\n",
      "30 103.43307495117188\n",
      "31 97.22976684570312\n",
      "32 91.37891387939453\n",
      "33 85.88009643554688\n",
      "34 80.70825958251953\n",
      "35 75.85533142089844\n",
      "36 71.29464721679688\n",
      "37 67.01573181152344\n",
      "38 63.00053024291992\n",
      "39 59.232120513916016\n",
      "40 55.7006950378418\n",
      "41 52.38496398925781\n",
      "42 49.27217102050781\n",
      "43 46.351600646972656\n",
      "44 43.61233139038086\n",
      "45 41.04492950439453\n",
      "46 38.63540267944336\n",
      "47 36.3797721862793\n",
      "48 34.263614654541016\n",
      "49 32.27854537963867\n",
      "50 30.418109893798828\n",
      "51 28.675573348999023\n",
      "52 27.040359497070312\n",
      "53 25.50742530822754\n",
      "54 24.068483352661133\n",
      "55 22.717262268066406\n",
      "56 21.454317092895508\n",
      "57 20.268596649169922\n",
      "58 19.15485954284668\n",
      "59 18.107097625732422\n",
      "60 17.121461868286133\n",
      "61 16.195032119750977\n",
      "62 15.323431015014648\n",
      "63 14.503260612487793\n",
      "64 13.725380897521973\n",
      "65 12.993651390075684\n",
      "66 12.30472183227539\n",
      "67 11.656463623046875\n",
      "68 11.044851303100586\n",
      "69 10.468307495117188\n",
      "70 9.92473316192627\n",
      "71 9.412142753601074\n",
      "72 8.927467346191406\n",
      "73 8.469482421875\n",
      "74 8.037546157836914\n",
      "75 7.630088806152344\n",
      "76 7.245099067687988\n",
      "77 6.880895137786865\n",
      "78 6.536797046661377\n",
      "79 6.211469650268555\n",
      "80 5.903873920440674\n",
      "81 5.612159252166748\n",
      "82 5.336195945739746\n",
      "83 5.07472038269043\n",
      "84 4.827237129211426\n",
      "85 4.592899322509766\n",
      "86 4.370563983917236\n",
      "87 4.15950345993042\n",
      "88 3.9596025943756104\n",
      "89 3.7698750495910645\n",
      "90 3.5899510383605957\n",
      "91 3.419036865234375\n",
      "92 3.2567036151885986\n",
      "93 3.102675676345825\n",
      "94 2.956449031829834\n",
      "95 2.817727565765381\n",
      "96 2.6858952045440674\n",
      "97 2.560730218887329\n",
      "98 2.44183611869812\n",
      "99 2.3287596702575684\n",
      "100 2.2213551998138428\n",
      "101 2.1191246509552\n",
      "102 2.0220143795013428\n",
      "103 1.929567575454712\n",
      "104 1.8416041135787964\n",
      "105 1.757825255393982\n",
      "106 1.6781867742538452\n",
      "107 1.6023749113082886\n",
      "108 1.5301952362060547\n",
      "109 1.4614704847335815\n",
      "110 1.3963801860809326\n",
      "111 1.3344342708587646\n",
      "112 1.2754101753234863\n",
      "113 1.2191344499588013\n",
      "114 1.1654711961746216\n",
      "115 1.1142895221710205\n",
      "116 1.0654418468475342\n",
      "117 1.0189573764801025\n",
      "118 0.9745825529098511\n",
      "119 0.9322348833084106\n",
      "120 0.8918687701225281\n",
      "121 0.8534504771232605\n",
      "122 0.8167760968208313\n",
      "123 0.7817697525024414\n",
      "124 0.7484650611877441\n",
      "125 0.7167105674743652\n",
      "126 0.686407744884491\n",
      "127 0.6575032472610474\n",
      "128 0.6298772692680359\n",
      "129 0.6034983396530151\n",
      "130 0.5782871842384338\n",
      "131 0.5541948080062866\n",
      "132 0.5311901569366455\n",
      "133 0.5092275738716125\n",
      "134 0.488243043422699\n",
      "135 0.4681360423564911\n",
      "136 0.4489268660545349\n",
      "137 0.4305775761604309\n",
      "138 0.41301894187927246\n",
      "139 0.3962223529815674\n",
      "140 0.38015925884246826\n",
      "141 0.3647973835468292\n",
      "142 0.35008421540260315\n",
      "143 0.3360048532485962\n",
      "144 0.3225302994251251\n",
      "145 0.30963772535324097\n",
      "146 0.29728996753692627\n",
      "147 0.2854713797569275\n",
      "148 0.27414166927337646\n",
      "149 0.26330992579460144\n",
      "150 0.25293678045272827\n",
      "151 0.2429889440536499\n",
      "152 0.2334655523300171\n",
      "153 0.2243337631225586\n",
      "154 0.21559585630893707\n",
      "155 0.20721504092216492\n",
      "156 0.19918997585773468\n",
      "157 0.19149312376976013\n",
      "158 0.18410825729370117\n",
      "159 0.17702895402908325\n",
      "160 0.1702398657798767\n",
      "161 0.16373787820339203\n",
      "162 0.15750271081924438\n",
      "163 0.15151599049568176\n",
      "164 0.1457705795764923\n",
      "165 0.1402616947889328\n",
      "166 0.1349755823612213\n",
      "167 0.1299021691083908\n",
      "168 0.1250341683626175\n",
      "169 0.12036570906639099\n",
      "170 0.11588458716869354\n",
      "171 0.11158397048711777\n",
      "172 0.10745933651924133\n",
      "173 0.10349902510643005\n",
      "174 0.09969352930784225\n",
      "175 0.09603467583656311\n",
      "176 0.0925242081284523\n",
      "177 0.08914954960346222\n",
      "178 0.08590707927942276\n",
      "179 0.08279795944690704\n",
      "180 0.07980289310216904\n",
      "181 0.0769294947385788\n",
      "182 0.07416914403438568\n",
      "183 0.07151540368795395\n",
      "184 0.06896471977233887\n",
      "185 0.06651239097118378\n",
      "186 0.06415601819753647\n",
      "187 0.06188745051622391\n",
      "188 0.05970660224556923\n",
      "189 0.05760873481631279\n",
      "190 0.0555911548435688\n",
      "191 0.0536472424864769\n",
      "192 0.0517776794731617\n",
      "193 0.04997703805565834\n",
      "194 0.04824415221810341\n",
      "195 0.046576015651226044\n",
      "196 0.04497308284044266\n",
      "197 0.043426066637039185\n",
      "198 0.04193766787648201\n",
      "199 0.040503744035959244\n",
      "200 0.039122261106967926\n",
      "201 0.0377911739051342\n",
      "202 0.0365101583302021\n",
      "203 0.03527547046542168\n",
      "204 0.03408518806099892\n",
      "205 0.03293826803565025\n",
      "206 0.03183213993906975\n",
      "207 0.0307663232088089\n",
      "208 0.029738016426563263\n",
      "209 0.02874712646007538\n",
      "210 0.027791960164904594\n",
      "211 0.026870645582675934\n",
      "212 0.025982240214943886\n",
      "213 0.025126703083515167\n",
      "214 0.02430015429854393\n",
      "215 0.023502809926867485\n",
      "216 0.022734621539711952\n",
      "217 0.02199268713593483\n",
      "218 0.02127729542553425\n",
      "219 0.02058619260787964\n",
      "220 0.01992025226354599\n",
      "221 0.0192763302475214\n",
      "222 0.018654413521289825\n",
      "223 0.018054792657494545\n",
      "224 0.017476065084338188\n",
      "225 0.016916483640670776\n",
      "226 0.016376446932554245\n",
      "227 0.01585562713444233\n",
      "228 0.015352350659668446\n",
      "229 0.014866185374557972\n",
      "230 0.014396720565855503\n",
      "231 0.013943132944405079\n",
      "232 0.013504460453987122\n",
      "233 0.013080769218504429\n",
      "234 0.012671305797994137\n",
      "235 0.012275774031877518\n",
      "236 0.011893372051417828\n",
      "237 0.011523507535457611\n",
      "238 0.011166385374963284\n",
      "239 0.010820452123880386\n",
      "240 0.010486094281077385\n",
      "241 0.010162830352783203\n",
      "242 0.00985085591673851\n",
      "243 0.00954875536262989\n",
      "244 0.009256602264940739\n",
      "245 0.008973592892289162\n",
      "246 0.008699934929609299\n",
      "247 0.008435212075710297\n",
      "248 0.008179167285561562\n",
      "249 0.007931368425488472\n",
      "250 0.007691506762057543\n",
      "251 0.0074593424797058105\n",
      "252 0.007234856486320496\n",
      "253 0.007017559837549925\n",
      "254 0.006807232275605202\n",
      "255 0.006603485904633999\n",
      "256 0.006406297907233238\n",
      "257 0.0062153092585504055\n",
      "258 0.006030414253473282\n",
      "259 0.005851340480148792\n",
      "260 0.005677908658981323\n",
      "261 0.005509968847036362\n",
      "262 0.005347406957298517\n",
      "263 0.0051897792145609856\n",
      "264 0.005037178285419941\n",
      "265 0.004889342468231916\n",
      "266 0.0047460575588047504\n",
      "267 0.004607275128364563\n",
      "268 0.004472812172025442\n",
      "269 0.004342454951256514\n",
      "270 0.0042160977609455585\n",
      "271 0.0040937489829957485\n",
      "272 0.003975162748247385\n",
      "273 0.0038601343985646963\n",
      "274 0.003748635994270444\n",
      "275 0.0036406032741069794\n",
      "276 0.0035358245950192213\n",
      "277 0.003434218466281891\n",
      "278 0.003335761371999979\n",
      "279 0.0032402663491666317\n",
      "280 0.0031476798467338085\n",
      "281 0.003057836089283228\n",
      "282 0.0029707481153309345\n",
      "283 0.002886275527998805\n",
      "284 0.002804344519972801\n",
      "285 0.0027248188853263855\n",
      "286 0.0026477244682610035\n",
      "287 0.0025729327462613583\n",
      "288 0.002500401111319661\n",
      "289 0.0024300043005496264\n",
      "290 0.002361641963943839\n",
      "291 0.0022953436709940434\n",
      "292 0.0022310176864266396\n",
      "293 0.002168601145967841\n",
      "294 0.0021079762373119593\n",
      "295 0.0020492044277489185\n",
      "296 0.0019920915365219116\n",
      "297 0.001936652697622776\n",
      "298 0.0018828518223017454\n",
      "299 0.0018305997364223003\n",
      "300 0.0017799206543713808\n",
      "301 0.0017306910594925284\n",
      "302 0.0016829072264954448\n",
      "303 0.0016364884795621037\n",
      "304 0.0015914163086563349\n",
      "305 0.0015476695261895657\n",
      "306 0.0015051582595333457\n",
      "307 0.0014638580614700913\n",
      "308 0.0014237493742257357\n",
      "309 0.0013847742229700089\n",
      "310 0.0013469037367030978\n",
      "311 0.0013100961223244667\n",
      "312 0.001274340902455151\n",
      "313 0.0012396360980346799\n",
      "314 0.0012059243163093925\n",
      "315 0.0011731695849448442\n",
      "316 0.001141335698775947\n",
      "317 0.0011103993747383356\n",
      "318 0.001080348389223218\n",
      "319 0.0010511359432712197\n",
      "320 0.0010227605234831572\n",
      "321 0.000995189999230206\n",
      "322 0.0009683995158411562\n",
      "323 0.0009423494338989258\n",
      "324 0.0009170320117846131\n",
      "325 0.0008924229769036174\n",
      "326 0.0008685190114192665\n",
      "327 0.0008452687179669738\n",
      "328 0.0008226630743592978\n",
      "329 0.0008006917778402567\n",
      "330 0.0007793357945047319\n",
      "331 0.0007585702114738524\n",
      "332 0.0007383747142739594\n",
      "333 0.000718742492608726\n",
      "334 0.0006996593438088894\n",
      "335 0.000681090634316206\n",
      "336 0.0006630563293583691\n",
      "337 0.0006455015391111374\n",
      "338 0.0006284286500886083\n",
      "339 0.0006118355086073279\n",
      "340 0.000595686084125191\n",
      "341 0.0005799764767289162\n",
      "342 0.0005647169891744852\n",
      "343 0.0005498573300428689\n",
      "344 0.0005354027962312102\n",
      "345 0.0005213433760218322\n",
      "346 0.0005076644592918456\n",
      "347 0.0004943592357449234\n",
      "348 0.0004814229905605316\n",
      "349 0.000468832062324509\n",
      "350 0.000456581445178017\n",
      "351 0.0004446710809133947\n",
      "352 0.00043306860607117414\n",
      "353 0.0004217869136482477\n",
      "354 0.0004108068242203444\n",
      "355 0.0004001168708782643\n",
      "356 0.0003897209244314581\n",
      "357 0.000379590957891196\n",
      "358 0.00036975854891352355\n",
      "359 0.0003601681673899293\n",
      "360 0.000350851594703272\n",
      "361 0.00034176564076915383\n",
      "362 0.0003329272149130702\n",
      "363 0.0003243301180191338\n",
      "364 0.00031595592736266553\n",
      "365 0.00030781186069361866\n",
      "366 0.0002998794661834836\n",
      "367 0.0002921617415267974\n",
      "368 0.0002846371498890221\n",
      "369 0.0002773238520603627\n",
      "370 0.0002701948687899858\n",
      "371 0.0002632635005284101\n",
      "372 0.00025651135365478694\n",
      "373 0.00024994200794026256\n",
      "374 0.0002435381175018847\n",
      "375 0.00023730183602310717\n",
      "376 0.00023124054132495075\n",
      "377 0.0002253316924907267\n",
      "378 0.0002195840934291482\n",
      "379 0.00021397594537120312\n",
      "380 0.0002085227897623554\n",
      "381 0.0002032069314736873\n",
      "382 0.00019803395844064653\n",
      "383 0.00019299636187497526\n",
      "384 0.00018809447647072375\n",
      "385 0.00018331033061258495\n",
      "386 0.00017865432891994715\n",
      "387 0.0001741229061735794\n",
      "388 0.00016970631259027869\n",
      "389 0.0001654055085964501\n",
      "390 0.0001612125342944637\n",
      "391 0.0001571381581015885\n",
      "392 0.00015316373901441693\n",
      "393 0.00014929231838323176\n",
      "394 0.00014551964704878628\n",
      "395 0.00014184681640472263\n",
      "396 0.0001382638147333637\n",
      "397 0.00013478296750690788\n",
      "398 0.00013138490612618625\n",
      "399 0.00012807390885427594\n",
      "400 0.00012485359911806881\n",
      "401 0.00012171261187177151\n",
      "402 0.00011865032865898684\n",
      "403 0.00011567271576495841\n",
      "404 0.00011276625446043909\n",
      "405 0.00010993715113727376\n",
      "406 0.000107181491330266\n",
      "407 0.00010449258115841076\n",
      "408 0.00010187449515797198\n",
      "409 9.93227367871441e-05\n",
      "410 9.683738608146086e-05\n",
      "411 9.441739530302584e-05\n",
      "412 9.205154492519796e-05\n",
      "413 8.975094533525407e-05\n",
      "414 8.751187851885334e-05\n",
      "415 8.532514038961381e-05\n",
      "416 8.319751941598952e-05\n",
      "417 8.11260542832315e-05\n",
      "418 7.909936539363116e-05\n",
      "419 7.713000377407297e-05\n",
      "420 7.521319639636204e-05\n",
      "421 7.333750545512885e-05\n",
      "422 7.151232421165332e-05\n",
      "423 6.973707786528394e-05\n",
      "424 6.800369010306895e-05\n",
      "425 6.631237192777917e-05\n",
      "426 6.466741615440696e-05\n",
      "427 6.306030263658613e-05\n",
      "428 6.14932068856433e-05\n",
      "429 5.9975060139549896e-05\n",
      "430 5.848788350704126e-05\n",
      "431 5.703813076252118e-05\n",
      "432 5.5629370763199404e-05\n",
      "433 5.425113340606913e-05\n",
      "434 5.2907085773767903e-05\n",
      "435 5.159985084901564e-05\n",
      "436 5.0325135816819966e-05\n",
      "437 4.90825368615333e-05\n",
      "438 4.786971476278268e-05\n",
      "439 4.668842302635312e-05\n",
      "440 4.553420876618475e-05\n",
      "441 4.441023338586092e-05\n",
      "442 4.331602031015791e-05\n",
      "443 4.224792792228982e-05\n",
      "444 4.1205370507668704e-05\n",
      "445 4.0190836443798617e-05\n",
      "446 3.920184099115431e-05\n",
      "447 3.823657243628986e-05\n",
      "448 3.729604941327125e-05\n",
      "449 3.637909685494378e-05\n",
      "450 3.5486962588038296e-05\n",
      "451 3.461278902250342e-05\n",
      "452 3.376079985173419e-05\n",
      "453 3.29306822095532e-05\n",
      "454 3.212330921087414e-05\n",
      "455 3.1335323001258075e-05\n",
      "456 3.056466084672138e-05\n",
      "457 2.981840771099087e-05\n",
      "458 2.9086031645420007e-05\n",
      "459 2.8372549422783777e-05\n",
      "460 2.7676533136400394e-05\n",
      "461 2.6999468900612555e-05\n",
      "462 2.6338972020312212e-05\n",
      "463 2.56933672062587e-05\n",
      "464 2.50659286393784e-05\n",
      "465 2.445215795887634e-05\n",
      "466 2.3855691324570216e-05\n",
      "467 2.3270849851542152e-05\n",
      "468 2.2701324269291945e-05\n",
      "469 2.2147134586703032e-05\n",
      "470 2.1605832444038242e-05\n",
      "471 2.107902582793031e-05\n",
      "472 2.0565234081004746e-05\n",
      "473 2.0063929696334526e-05\n",
      "474 1.957415952347219e-05\n",
      "475 1.9095949028269388e-05\n",
      "476 1.863063152995892e-05\n",
      "477 1.817787233449053e-05\n",
      "478 1.7733675122144632e-05\n",
      "479 1.7301830666838214e-05\n",
      "480 1.687953954387922e-05\n",
      "481 1.6470803529955447e-05\n",
      "482 1.606777004781179e-05\n",
      "483 1.5677069313824177e-05\n",
      "484 1.529566361568868e-05\n",
      "485 1.4924703464203048e-05\n",
      "486 1.4561932403012179e-05\n",
      "487 1.4206372725311667e-05\n",
      "488 1.3861443221685477e-05\n",
      "489 1.3524547284760047e-05\n",
      "490 1.3196960026107263e-05\n",
      "491 1.2875205356976949e-05\n",
      "492 1.2562719348352402e-05\n",
      "493 1.22584633572842e-05\n",
      "494 1.1960227311647031e-05\n",
      "495 1.1671733773255255e-05\n",
      "496 1.1386536243662704e-05\n",
      "497 1.1112037100247107e-05\n",
      "498 1.0841327821253799e-05\n",
      "499 1.0578757610346656e-05\n"
     ]
    }
   ],
   "source": [
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "print(x.requires_grad)\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Variables for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Variable of input data to the Module and it produces\n",
    "    # a Variable of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Variables containing the predicted and true\n",
    "    # values of y, and the loss function returns a Variable containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Variables with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Variable, so\n",
    "    # we can access its data and gradients like we did before.\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard model wrapper\n",
    "We have seen how nn.Module can help us simplify our code. We can also use that to wrap around arbitrary number of operations to create a model. Using nn.module also allows us to use more complicated pre-built optimizers (as oppose to just looping through params and manually doing naive SGD).\n",
    "\n",
    "This becomes our bread-and-butter way of writing models in PyTorch: using a nn.Module wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0392284393310547\n",
      "1 1.027318000793457\n",
      "2 1.0156762599945068\n",
      "3 1.0043089389801025\n",
      "4 0.9931926727294922\n",
      "5 0.9823950529098511\n",
      "6 0.9718162417411804\n",
      "7 0.9614356756210327\n",
      "8 0.9512289762496948\n",
      "9 0.9411896467208862\n",
      "10 0.931373119354248\n",
      "11 0.9217436909675598\n",
      "12 0.9122875928878784\n",
      "13 0.9030243754386902\n",
      "14 0.8939418792724609\n",
      "15 0.8849756121635437\n",
      "16 0.8761796951293945\n",
      "17 0.8675147294998169\n",
      "18 0.8589730262756348\n",
      "19 0.8505562543869019\n",
      "20 0.8422958254814148\n",
      "21 0.8341854810714722\n",
      "22 0.8262068033218384\n",
      "23 0.8183488845825195\n",
      "24 0.8105946779251099\n",
      "25 0.8029730916023254\n",
      "26 0.7954890727996826\n",
      "27 0.7881265878677368\n",
      "28 0.7808834314346313\n",
      "29 0.773739755153656\n",
      "30 0.7666952610015869\n",
      "31 0.759775698184967\n",
      "32 0.75295090675354\n",
      "33 0.7462307214736938\n",
      "34 0.739641547203064\n",
      "35 0.7331834435462952\n",
      "36 0.7268016338348389\n",
      "37 0.7205035090446472\n",
      "38 0.7142890691757202\n",
      "39 0.7081447839736938\n",
      "40 0.7020708322525024\n",
      "41 0.696076512336731\n",
      "42 0.6901798844337463\n",
      "43 0.6843438744544983\n",
      "44 0.6785628199577332\n",
      "45 0.6728572249412537\n",
      "46 0.6672331094741821\n",
      "47 0.6616730690002441\n",
      "48 0.6561798453330994\n",
      "49 0.6507624387741089\n",
      "50 0.6453968286514282\n",
      "51 0.640062689781189\n",
      "52 0.6347851753234863\n",
      "53 0.6295517683029175\n",
      "54 0.6243737936019897\n",
      "55 0.6192370653152466\n",
      "56 0.6141596436500549\n",
      "57 0.6091282367706299\n",
      "58 0.6041378378868103\n",
      "59 0.5992053747177124\n",
      "60 0.5943084955215454\n",
      "61 0.5894574522972107\n",
      "62 0.5846618413925171\n",
      "63 0.5799142122268677\n",
      "64 0.575206458568573\n",
      "65 0.57054603099823\n",
      "66 0.5659251809120178\n",
      "67 0.5613447427749634\n",
      "68 0.5568181872367859\n",
      "69 0.5523321032524109\n",
      "70 0.547894299030304\n",
      "71 0.54350745677948\n",
      "72 0.5391522645950317\n",
      "73 0.5348380208015442\n",
      "74 0.5305480360984802\n",
      "75 0.526290237903595\n",
      "76 0.5220779776573181\n",
      "77 0.5179155468940735\n",
      "78 0.5137916803359985\n",
      "79 0.5097037553787231\n",
      "80 0.5056559443473816\n",
      "81 0.5016461610794067\n",
      "82 0.49767112731933594\n",
      "83 0.4937187731266022\n",
      "84 0.48980969190597534\n",
      "85 0.4859350621700287\n",
      "86 0.482089102268219\n",
      "87 0.47827520966529846\n",
      "88 0.4744856357574463\n",
      "89 0.4707280695438385\n",
      "90 0.4669983386993408\n",
      "91 0.46329325437545776\n",
      "92 0.45962396264076233\n",
      "93 0.4559803605079651\n",
      "94 0.4523589015007019\n",
      "95 0.4487658441066742\n",
      "96 0.445194810628891\n",
      "97 0.44165197014808655\n",
      "98 0.43813538551330566\n",
      "99 0.4346480369567871\n",
      "100 0.43118125200271606\n",
      "101 0.42773500084877014\n",
      "102 0.42431536316871643\n",
      "103 0.4209139347076416\n",
      "104 0.4175301194190979\n",
      "105 0.4141763746738434\n",
      "106 0.4108506739139557\n",
      "107 0.4075527787208557\n",
      "108 0.40427884459495544\n",
      "109 0.4010260999202728\n",
      "110 0.39779967069625854\n",
      "111 0.3945998549461365\n",
      "112 0.39142268896102905\n",
      "113 0.38826408982276917\n",
      "114 0.38512980937957764\n",
      "115 0.3820139765739441\n",
      "116 0.3789205253124237\n",
      "117 0.37584906816482544\n",
      "118 0.3727990686893463\n",
      "119 0.3697686791419983\n",
      "120 0.36675751209259033\n",
      "121 0.36376482248306274\n",
      "122 0.3607913851737976\n",
      "123 0.3578406274318695\n",
      "124 0.3549099564552307\n",
      "125 0.3519945442676544\n",
      "126 0.34910205006599426\n",
      "127 0.3462275266647339\n",
      "128 0.343370258808136\n",
      "129 0.3405340015888214\n",
      "130 0.3377158045768738\n",
      "131 0.33491426706314087\n",
      "132 0.33212971687316895\n",
      "133 0.32936903834342957\n",
      "134 0.32662779092788696\n",
      "135 0.3239070177078247\n",
      "136 0.3211955428123474\n",
      "137 0.3185034394264221\n",
      "138 0.31582921743392944\n",
      "139 0.31317463517189026\n",
      "140 0.31053775548934937\n",
      "141 0.30791574716567993\n",
      "142 0.30531567335128784\n",
      "143 0.3027384877204895\n",
      "144 0.30017799139022827\n",
      "145 0.297637403011322\n",
      "146 0.29511481523513794\n",
      "147 0.29261526465415955\n",
      "148 0.2901369035243988\n",
      "149 0.2876710295677185\n",
      "150 0.2852206528186798\n",
      "151 0.2827944755554199\n",
      "152 0.2803836762905121\n",
      "153 0.27799054980278015\n",
      "154 0.2756151854991913\n",
      "155 0.2732568383216858\n",
      "156 0.27091509103775024\n",
      "157 0.26858940720558167\n",
      "158 0.2662866711616516\n",
      "159 0.26400208473205566\n",
      "160 0.26173603534698486\n",
      "161 0.25948646664619446\n",
      "162 0.2572522759437561\n",
      "163 0.25503554940223694\n",
      "164 0.2528296113014221\n",
      "165 0.2506422698497772\n",
      "166 0.24847090244293213\n",
      "167 0.24631397426128387\n",
      "168 0.24417409300804138\n",
      "169 0.242049902677536\n",
      "170 0.2399425059556961\n",
      "171 0.23785372078418732\n",
      "172 0.23578055202960968\n",
      "173 0.23372595012187958\n",
      "174 0.2316858470439911\n",
      "175 0.22966261208057404\n",
      "176 0.22765390574932098\n",
      "177 0.22565916180610657\n",
      "178 0.2236821949481964\n",
      "179 0.2217199057340622\n",
      "180 0.21977099776268005\n",
      "181 0.21783873438835144\n",
      "182 0.21592167019844055\n",
      "183 0.21401937305927277\n",
      "184 0.21213233470916748\n",
      "185 0.21026000380516052\n",
      "186 0.20840008556842804\n",
      "187 0.2065574675798416\n",
      "188 0.20472827553749084\n",
      "189 0.20291240513324738\n",
      "190 0.20111103355884552\n",
      "191 0.19932803511619568\n",
      "192 0.19755716621875763\n",
      "193 0.1958031952381134\n",
      "194 0.1940615475177765\n",
      "195 0.19233103096485138\n",
      "196 0.1906122863292694\n",
      "197 0.18890587985515594\n",
      "198 0.18721415102481842\n",
      "199 0.1855379045009613\n",
      "200 0.18387524783611298\n",
      "201 0.18222664296627045\n",
      "202 0.18058905005455017\n",
      "203 0.17896559834480286\n",
      "204 0.1773548573255539\n",
      "205 0.17575909197330475\n",
      "206 0.17417538166046143\n",
      "207 0.1726040244102478\n",
      "208 0.17104828357696533\n",
      "209 0.16950498521327972\n",
      "210 0.16797247529029846\n",
      "211 0.1664542555809021\n",
      "212 0.164951354265213\n",
      "213 0.1634587198495865\n",
      "214 0.16197888553142548\n",
      "215 0.16051432490348816\n",
      "216 0.1590614765882492\n",
      "217 0.15761908888816833\n",
      "218 0.156189426779747\n",
      "219 0.1547747552394867\n",
      "220 0.15336939692497253\n",
      "221 0.15197980403900146\n",
      "222 0.15060308575630188\n",
      "223 0.14923742413520813\n",
      "224 0.14788571000099182\n",
      "225 0.1465432494878769\n",
      "226 0.14521297812461853\n",
      "227 0.14389337599277496\n",
      "228 0.14258497953414917\n",
      "229 0.1412878930568695\n",
      "230 0.1400032937526703\n",
      "231 0.13873086869716644\n",
      "232 0.1374705731868744\n",
      "233 0.13622018694877625\n",
      "234 0.13498274981975555\n",
      "235 0.13375529646873474\n",
      "236 0.13253717124462128\n",
      "237 0.13133089244365692\n",
      "238 0.1301361471414566\n",
      "239 0.12895157933235168\n",
      "240 0.12777796387672424\n",
      "241 0.12661299109458923\n",
      "242 0.12545914947986603\n",
      "243 0.1243143081665039\n",
      "244 0.1231786459684372\n",
      "245 0.1220552921295166\n",
      "246 0.12093911319971085\n",
      "247 0.1198347955942154\n",
      "248 0.11874010413885117\n",
      "249 0.1176532730460167\n",
      "250 0.11657693237066269\n",
      "251 0.11550966650247574\n",
      "252 0.11445178836584091\n",
      "253 0.11340268701314926\n",
      "254 0.11236239969730377\n",
      "255 0.11133094877004623\n",
      "256 0.1103096753358841\n",
      "257 0.10929714143276215\n",
      "258 0.10829427093267441\n",
      "259 0.1073005199432373\n",
      "260 0.10631723701953888\n",
      "261 0.10534177720546722\n",
      "262 0.10437513887882233\n",
      "263 0.1034172773361206\n",
      "264 0.10246938467025757\n",
      "265 0.10152894258499146\n",
      "266 0.10059759765863419\n",
      "267 0.09967512637376785\n",
      "268 0.09876041859388351\n",
      "269 0.09785453975200653\n",
      "270 0.09695762395858765\n",
      "271 0.09607025235891342\n",
      "272 0.09518970549106598\n",
      "273 0.09431628882884979\n",
      "274 0.09345068782567978\n",
      "275 0.09259405732154846\n",
      "276 0.0917457863688469\n",
      "277 0.09090427309274673\n",
      "278 0.0900663509964943\n",
      "279 0.08923637866973877\n",
      "280 0.08841454237699509\n",
      "281 0.08760057389736176\n",
      "282 0.08679370582103729\n",
      "283 0.08599450439214706\n",
      "284 0.08520245552062988\n",
      "285 0.08441852033138275\n",
      "286 0.08364167809486389\n",
      "287 0.08287220448255539\n",
      "288 0.08211005479097366\n",
      "289 0.08135513961315155\n",
      "290 0.08060742169618607\n",
      "291 0.07986623793840408\n",
      "292 0.07913221418857574\n",
      "293 0.07840551435947418\n",
      "294 0.07768594473600388\n",
      "295 0.07697295397520065\n",
      "296 0.07626678049564362\n",
      "297 0.075566865503788\n",
      "298 0.07487413287162781\n",
      "299 0.07418853789567947\n",
      "300 0.07350878417491913\n",
      "301 0.07283520698547363\n",
      "302 0.07216741144657135\n",
      "303 0.07150651514530182\n",
      "304 0.0708521381020546\n",
      "305 0.07020439207553864\n",
      "306 0.06956267356872559\n",
      "307 0.06892693787813187\n",
      "308 0.06829676777124405\n",
      "309 0.06767264753580093\n",
      "310 0.06705440580844879\n",
      "311 0.0664428099989891\n",
      "312 0.06583648175001144\n",
      "313 0.0652347132563591\n",
      "314 0.06463854014873505\n",
      "315 0.06404851377010345\n",
      "316 0.06346457451581955\n",
      "317 0.06288580596446991\n",
      "318 0.062312446534633636\n",
      "319 0.06174462288618088\n",
      "320 0.061182547360658646\n",
      "321 0.060625262558460236\n",
      "322 0.06007316708564758\n",
      "323 0.059526991099119186\n",
      "324 0.058985382318496704\n",
      "325 0.05844855308532715\n",
      "326 0.05791648477315903\n",
      "327 0.057390082627534866\n",
      "328 0.05686946585774422\n",
      "329 0.056353263556957245\n",
      "330 0.05584265664219856\n",
      "331 0.055336952209472656\n",
      "332 0.054835379123687744\n",
      "333 0.05433792620897293\n",
      "334 0.053845666348934174\n",
      "335 0.05335800722241402\n",
      "336 0.0528746135532856\n",
      "337 0.052396416664123535\n",
      "338 0.051922865211963654\n",
      "339 0.051453787833452225\n",
      "340 0.0509895458817482\n",
      "341 0.05052884668111801\n",
      "342 0.050073135644197464\n",
      "343 0.049621693789958954\n",
      "344 0.04917478561401367\n",
      "345 0.048731494694948196\n",
      "346 0.0482923798263073\n",
      "347 0.047857291996479034\n",
      "348 0.04742638021707535\n",
      "349 0.046999454498291016\n",
      "350 0.04657677561044693\n",
      "351 0.046157944947481155\n",
      "352 0.0457431823015213\n",
      "353 0.04532709717750549\n",
      "354 0.044914789497852325\n",
      "355 0.044506557285785675\n",
      "356 0.04410197585821152\n",
      "357 0.04370158910751343\n",
      "358 0.043305180966854095\n",
      "359 0.042912475764751434\n",
      "360 0.04252336174249649\n",
      "361 0.04213781654834747\n",
      "362 0.04175630211830139\n",
      "363 0.04137816280126572\n",
      "364 0.04100419580936432\n",
      "365 0.040634460747241974\n",
      "366 0.04026853293180466\n",
      "367 0.03990583494305611\n",
      "368 0.03954646736383438\n",
      "369 0.03919079527258873\n",
      "370 0.03883896768093109\n",
      "371 0.03849048912525177\n",
      "372 0.03814581409096718\n",
      "373 0.037804268300533295\n",
      "374 0.037466175854206085\n",
      "375 0.03713121637701988\n",
      "376 0.03679925203323364\n",
      "377 0.036470748484134674\n",
      "378 0.03614533692598343\n",
      "379 0.035822976380586624\n",
      "380 0.03550327196717262\n",
      "381 0.03518681973218918\n",
      "382 0.03487413376569748\n",
      "383 0.03456404432654381\n",
      "384 0.03425675630569458\n",
      "385 0.03395233675837517\n",
      "386 0.03365079313516617\n",
      "387 0.03335217386484146\n",
      "388 0.03305651992559433\n",
      "389 0.03276349604129791\n",
      "390 0.03247315436601639\n",
      "391 0.03218584135174751\n",
      "392 0.031901367008686066\n",
      "393 0.03161843866109848\n",
      "394 0.03133820742368698\n",
      "395 0.03106069564819336\n",
      "396 0.030785584822297096\n",
      "397 0.030512934550642967\n",
      "398 0.030243132263422012\n",
      "399 0.029975950717926025\n",
      "400 0.029711240902543068\n",
      "401 0.02944907546043396\n",
      "402 0.029189446941018105\n",
      "403 0.02893209084868431\n",
      "404 0.028677325695753098\n",
      "405 0.028424914926290512\n",
      "406 0.028174782171845436\n",
      "407 0.02792702242732048\n",
      "408 0.027681684121489525\n",
      "409 0.027438830584287643\n",
      "410 0.027198035269975662\n",
      "411 0.026959437876939774\n",
      "412 0.0267232246696949\n",
      "413 0.02648901380598545\n",
      "414 0.026257237419486046\n",
      "415 0.02602793648838997\n",
      "416 0.025800451636314392\n",
      "417 0.02557489648461342\n",
      "418 0.025351867079734802\n",
      "419 0.02513047680258751\n",
      "420 0.024911213666200638\n",
      "421 0.024693969637155533\n",
      "422 0.02447883039712906\n",
      "423 0.024265820160508156\n",
      "424 0.02405492402613163\n",
      "425 0.02384570613503456\n",
      "426 0.023638753220438957\n",
      "427 0.02343342825770378\n",
      "428 0.02323000505566597\n",
      "429 0.023028701543807983\n",
      "430 0.0228293277323246\n",
      "431 0.022631904110312462\n",
      "432 0.022436197847127914\n",
      "433 0.02224224992096424\n",
      "434 0.022050153464078903\n",
      "435 0.021859925240278244\n",
      "436 0.021671514958143234\n",
      "437 0.021484844386577606\n",
      "438 0.02129991166293621\n",
      "439 0.02111705206334591\n",
      "440 0.020935630425810814\n",
      "441 0.020755957812070847\n",
      "442 0.02057804726064205\n",
      "443 0.020401686429977417\n",
      "444 0.020227108150720596\n",
      "445 0.020053911954164505\n",
      "446 0.01988239400088787\n",
      "447 0.019712431356310844\n",
      "448 0.019544098526239395\n",
      "449 0.01937735453248024\n",
      "450 0.019212042912840843\n",
      "451 0.01904713362455368\n",
      "452 0.01888367161154747\n",
      "453 0.018721800297498703\n",
      "454 0.018561214208602905\n",
      "455 0.0184023454785347\n",
      "456 0.01824491284787655\n",
      "457 0.018088966608047485\n",
      "458 0.017934506759047508\n",
      "459 0.017781734466552734\n",
      "460 0.01763032004237175\n",
      "461 0.01748007908463478\n",
      "462 0.017331335693597794\n",
      "463 0.017183978110551834\n",
      "464 0.017037848010659218\n",
      "465 0.01689297892153263\n",
      "466 0.016749534755945206\n",
      "467 0.01660746894776821\n",
      "468 0.016466759145259857\n",
      "469 0.016327453777194023\n",
      "470 0.016189200803637505\n",
      "471 0.016052305698394775\n",
      "472 0.015916816890239716\n",
      "473 0.015782328322529793\n",
      "474 0.01564915105700493\n",
      "475 0.015517259947955608\n",
      "476 0.01538676768541336\n",
      "477 0.01525737065821886\n",
      "478 0.015129053965210915\n",
      "479 0.015001880936324596\n",
      "480 0.014876140281558037\n",
      "481 0.014751464128494263\n",
      "482 0.014627845957875252\n",
      "483 0.014505403116345406\n",
      "484 0.014384174719452858\n",
      "485 0.014264064840972424\n",
      "486 0.014144917950034142\n",
      "487 0.014026793651282787\n",
      "488 0.013909799978137016\n",
      "489 0.013793954625725746\n",
      "490 0.01367921195924282\n",
      "491 0.01356535404920578\n",
      "492 0.013452587649226189\n",
      "493 0.013340826146304607\n",
      "494 0.013229976408183575\n",
      "495 0.013120265677571297\n",
      "496 0.013011468574404716\n",
      "497 0.012903623282909393\n",
      "498 0.01279684342443943\n",
      "499 0.012690866366028786\n"
     ]
    }
   ],
   "source": [
    "class twoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, hidden_size, D_out):\n",
    "        super(twoLayerNet, self).__init__() # ???\n",
    "        self.dense1 = torch.nn.Linear(D_in, hidden_size)\n",
    "        self.dense2 = torch.nn.Linear(hidden_size, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_out = torch.nn.ReLU()(self.dense1(x))\n",
    "        pred = self.dense2(h_out)\n",
    "        return pred\n",
    "\n",
    "model = twoLayerNet(D_in, hidden_size, D_out)\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "#Construct loss function and Optimizer.\n",
    "#model.parameters() returns all learnable parameters of the nn.Linear modules\n",
    "criterion = torch.nn.MSELoss(size_average=True)\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(epoch, loss.data[0])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0263596773147583\n",
      "1 0.9831148386001587\n",
      "2 0.9827384948730469\n",
      "3 0.9589020013809204\n",
      "4 0.9592604637145996\n",
      "5 1.020407795906067\n",
      "6 0.9803665280342102\n",
      "7 0.9587608575820923\n",
      "8 0.9787673950195312\n",
      "9 1.0124536752700806\n",
      "10 1.0095927715301514\n",
      "11 0.9580065608024597\n",
      "12 0.9749660491943359\n",
      "13 0.9991628527641296\n",
      "14 0.9571253657341003\n",
      "15 0.9573929905891418\n",
      "16 0.9709998965263367\n",
      "17 0.9570616483688354\n",
      "18 0.9560775756835938\n",
      "19 0.9799356460571289\n",
      "20 0.9555155038833618\n",
      "21 0.9734967350959778\n",
      "22 0.9696098566055298\n",
      "23 0.9649871587753296\n",
      "24 0.9558753967285156\n",
      "25 0.963191032409668\n",
      "26 0.9622885584831238\n",
      "27 0.9536253809928894\n",
      "28 0.9602544903755188\n",
      "29 0.9550212621688843\n",
      "30 0.9548455476760864\n",
      "31 0.9546681642532349\n",
      "32 0.956295371055603\n",
      "33 0.9552812576293945\n",
      "34 0.9517379999160767\n",
      "35 0.9514510035514832\n",
      "36 0.9511398077011108\n",
      "37 0.9229507446289062\n",
      "38 0.9504969716072083\n",
      "39 0.9501649737358093\n",
      "40 0.916441798210144\n",
      "41 0.9480305910110474\n",
      "42 0.9110943078994751\n",
      "43 0.9077723622322083\n",
      "44 0.9525190591812134\n",
      "45 0.9447699785232544\n",
      "46 0.9480806589126587\n",
      "47 0.893986701965332\n",
      "48 0.9422833323478699\n",
      "49 0.8870895504951477\n",
      "50 0.9470337629318237\n",
      "51 0.9467658996582031\n",
      "52 0.8764795064926147\n",
      "53 0.9382281303405762\n",
      "54 0.8692194223403931\n",
      "55 0.936567485332489\n",
      "56 0.9454444646835327\n",
      "57 0.9451756477355957\n",
      "58 0.9339653253555298\n",
      "59 0.9446004033088684\n",
      "60 0.8502181768417358\n",
      "61 0.9313567876815796\n",
      "62 0.9500330686569214\n",
      "63 0.8418172597885132\n",
      "64 0.8386549949645996\n",
      "65 0.9430215954780579\n",
      "66 0.927291989326477\n",
      "67 0.92645663022995\n",
      "68 0.8258244395256042\n",
      "69 0.9420787692070007\n",
      "70 0.9418339729309082\n",
      "71 0.9230439066886902\n",
      "72 0.9221620559692383\n",
      "73 0.9211624264717102\n",
      "74 0.9200572967529297\n",
      "75 0.9484785795211792\n",
      "76 0.9483561515808105\n",
      "77 0.8053117990493774\n",
      "78 0.9158893823623657\n",
      "79 0.9148828387260437\n",
      "80 0.939549446105957\n",
      "81 0.9127742052078247\n",
      "82 0.9391204714775085\n",
      "83 0.9388843774795532\n",
      "84 0.9386264085769653\n",
      "85 0.9383502006530762\n",
      "86 0.9471721649169922\n",
      "87 0.9074280858039856\n",
      "88 0.9375241994857788\n",
      "89 0.946821391582489\n",
      "90 0.9051544070243835\n",
      "91 0.7882929444313049\n",
      "92 0.9365050196647644\n",
      "93 0.9029071927070618\n",
      "94 0.9360181093215942\n",
      "95 0.7836077213287354\n",
      "96 0.9006656408309937\n",
      "97 0.9459466934204102\n",
      "98 0.7789689898490906\n",
      "99 0.8984493017196655\n",
      "100 0.7750878930091858\n",
      "101 0.9345289468765259\n",
      "102 0.9343307614326477\n",
      "103 0.8956422805786133\n",
      "104 0.7667158842086792\n",
      "105 0.7643405199050903\n",
      "106 0.9451001882553101\n",
      "107 0.9450117945671082\n",
      "108 0.8925076723098755\n",
      "109 0.8918539881706238\n",
      "110 0.752466082572937\n",
      "111 0.9327275156974792\n",
      "112 0.8897274136543274\n",
      "113 0.7458010911941528\n",
      "114 0.9443864822387695\n",
      "115 0.9442985653877258\n",
      "116 0.9442055821418762\n",
      "117 0.737158477306366\n",
      "118 0.7347972989082336\n",
      "119 0.9439319372177124\n",
      "120 0.9438422918319702\n",
      "121 0.9313740730285645\n",
      "122 0.9312364459037781\n",
      "123 0.8841099739074707\n",
      "124 0.8836240768432617\n",
      "125 0.9433749914169312\n",
      "126 0.943278968334198\n",
      "127 0.9304342269897461\n",
      "128 0.9302560687065125\n",
      "129 0.8809953927993774\n",
      "130 0.8804122805595398\n",
      "131 0.9296634793281555\n",
      "132 0.7130448222160339\n",
      "133 0.7117060422897339\n",
      "134 0.709818959236145\n",
      "135 0.9424554705619812\n",
      "136 0.7053128480911255\n",
      "137 0.8766109347343445\n",
      "138 0.8760719299316406\n",
      "139 0.8754060864448547\n",
      "140 0.9282469749450684\n",
      "141 0.928083062171936\n",
      "142 0.8732660412788391\n",
      "143 0.6918598413467407\n",
      "144 0.6899747848510742\n",
      "145 0.8711833953857422\n",
      "146 0.6855130195617676\n",
      "147 0.9271195530891418\n",
      "148 0.9269569516181946\n",
      "149 0.9414585828781128\n",
      "150 0.8680979013442993\n",
      "151 0.9413117170333862\n",
      "152 0.6736903190612793\n",
      "153 0.6717334985733032\n",
      "154 0.9259721636772156\n",
      "155 0.941026508808136\n",
      "156 0.9409515261650085\n",
      "157 0.9255107641220093\n",
      "158 0.9253355264663696\n",
      "159 0.9407066106796265\n",
      "160 0.9249439239501953\n",
      "161 0.9247293472290039\n",
      "162 0.8632014989852905\n",
      "163 0.8627837300300598\n",
      "164 0.9402718544006348\n",
      "165 0.8617285490036011\n",
      "166 0.940093994140625\n",
      "167 0.9400014877319336\n",
      "168 0.8600137829780579\n",
      "169 0.9398118853569031\n",
      "170 0.8587888479232788\n",
      "171 0.6518977284431458\n",
      "172 0.6509451270103455\n",
      "173 0.8568897247314453\n",
      "174 0.9393850564956665\n",
      "175 0.6469941139221191\n",
      "176 0.9392319917678833\n",
      "177 0.8545181155204773\n",
      "178 0.9219080209732056\n",
      "179 0.8533099889755249\n",
      "180 0.6402477025985718\n",
      "181 0.9388536214828491\n",
      "182 0.9387790560722351\n",
      "183 0.8509172201156616\n",
      "184 0.9386194348335266\n",
      "185 0.6339067220687866\n",
      "186 0.6324135065078735\n",
      "187 0.6304920315742493\n",
      "188 0.9207338094711304\n",
      "189 0.6261268854141235\n",
      "190 0.9205219149589539\n",
      "191 0.9203893542289734\n",
      "192 0.920224666595459\n",
      "193 0.6178418397903442\n",
      "194 0.9198621511459351\n",
      "195 0.919664204120636\n",
      "196 0.9378283619880676\n",
      "197 0.9377557635307312\n",
      "198 0.6091980338096619\n",
      "199 0.9376058578491211\n",
      "200 0.6057978868484497\n",
      "201 0.918552041053772\n",
      "202 0.9183778762817383\n",
      "203 0.6004269123077393\n",
      "204 0.9372503161430359\n",
      "205 0.5966733694076538\n",
      "206 0.917679488658905\n",
      "207 0.9370449185371399\n",
      "208 0.9369705319404602\n",
      "209 0.5893774628639221\n",
      "210 0.9170263409614563\n",
      "211 0.9367462396621704\n",
      "212 0.91669100522995\n",
      "213 0.9164980053901672\n",
      "214 0.5816108584403992\n",
      "215 0.9160860180854797\n",
      "216 0.8430673480033875\n",
      "217 0.8427876234054565\n",
      "218 0.5760542750358582\n",
      "219 0.5744915008544922\n",
      "220 0.9360928535461426\n",
      "221 0.8412987589836121\n",
      "222 0.8408273458480835\n",
      "223 0.5679394006729126\n",
      "224 0.9144800305366516\n",
      "225 0.9143086671829224\n",
      "226 0.9357113838195801\n",
      "227 0.9139161109924316\n",
      "228 0.9136972427368164\n",
      "229 0.837476909160614\n",
      "230 0.8369515538215637\n",
      "231 0.9353469610214233\n",
      "232 0.9127747416496277\n",
      "233 0.5567606091499329\n",
      "234 0.9123224020004272\n",
      "235 0.9120847582817078\n",
      "236 0.5538535714149475\n",
      "237 0.9115949869155884\n",
      "238 0.5514504909515381\n",
      "239 0.911117672920227\n",
      "240 0.9347015619277954\n",
      "241 0.5473906397819519\n",
      "242 0.9104277491569519\n",
      "243 0.8315873146057129\n",
      "244 0.5431458353996277\n",
      "245 0.5415307879447937\n",
      "246 0.8304882049560547\n",
      "247 0.8300117254257202\n",
      "248 0.8293997049331665\n",
      "249 0.5349345207214355\n",
      "250 0.9088977575302124\n",
      "251 0.827404797077179\n",
      "252 0.9339826703071594\n",
      "253 0.9083386659622192\n",
      "254 0.9081281423568726\n",
      "255 0.9337772130966187\n",
      "256 0.9336984753608704\n",
      "257 0.9074527621269226\n",
      "258 0.524543285369873\n",
      "259 0.907000720500946\n",
      "260 0.9067603945732117\n",
      "261 0.8222328424453735\n",
      "262 0.9332081079483032\n",
      "263 0.9331213235855103\n",
      "264 0.9057685136795044\n",
      "265 0.5188034772872925\n",
      "266 0.5178197026252747\n",
      "267 0.819744884967804\n",
      "268 0.9048931002616882\n",
      "269 0.9046651721000671\n",
      "270 0.9044103622436523\n",
      "271 0.932468593120575\n",
      "272 0.932381272315979\n",
      "273 0.5108897089958191\n",
      "274 0.5098093748092651\n",
      "275 0.9321317672729492\n",
      "276 0.8164475560188293\n",
      "277 0.8160282373428345\n",
      "278 0.9026710391044617\n",
      "279 0.9024540185928345\n",
      "280 0.5031676292419434\n",
      "281 0.9019899368286133\n",
      "282 0.93156898021698\n",
      "283 0.4999440312385559\n",
      "284 0.9013044238090515\n",
      "285 0.8125091791152954\n",
      "286 0.9008359909057617\n",
      "287 0.8115617036819458\n",
      "288 0.8109637498855591\n",
      "289 0.8102415800094604\n",
      "290 0.8094085454940796\n",
      "291 0.9308375120162964\n",
      "292 0.8993736505508423\n",
      "293 0.8068641424179077\n",
      "294 0.49098101258277893\n",
      "295 0.8051910400390625\n",
      "296 0.48935216665267944\n",
      "297 0.9303345680236816\n",
      "298 0.8980592489242554\n",
      "299 0.4862968325614929\n",
      "300 0.9300959706306458\n",
      "301 0.4839223027229309\n",
      "302 0.8003549575805664\n",
      "303 0.929864764213562\n",
      "304 0.7991165518760681\n",
      "305 0.8967905044555664\n",
      "306 0.7977410554885864\n",
      "307 0.4771706163883209\n",
      "308 0.7962542772293091\n",
      "309 0.8960024118423462\n",
      "310 0.7946775555610657\n",
      "311 0.4730716645717621\n",
      "312 0.7930260896682739\n",
      "313 0.8951505422592163\n",
      "314 0.7913100719451904\n",
      "315 0.4689215123653412\n",
      "316 0.928824782371521\n",
      "317 0.4666237235069275\n",
      "318 0.9286708831787109\n",
      "319 0.9285892248153687\n",
      "320 0.8937265276908875\n",
      "321 0.8935149908065796\n",
      "322 0.9283056259155273\n",
      "323 0.7853285670280457\n",
      "324 0.9280973672866821\n",
      "325 0.8925716280937195\n",
      "326 0.4582134783267975\n",
      "327 0.9277765154838562\n",
      "328 0.8918584585189819\n",
      "329 0.8916026949882507\n",
      "330 0.781886637210846\n",
      "331 0.781337559223175\n",
      "332 0.9272302389144897\n",
      "333 0.9271157383918762\n",
      "334 0.45294705033302307\n",
      "335 0.7789537310600281\n",
      "336 0.8897908926010132\n",
      "337 0.4507623612880707\n",
      "338 0.7771350145339966\n",
      "339 0.4488774240016937\n",
      "340 0.7758175730705261\n",
      "341 0.44658130407333374\n",
      "342 0.8884469866752625\n",
      "343 0.9261409640312195\n",
      "344 0.7731460332870483\n",
      "345 0.8877660632133484\n",
      "346 0.8875024914741516\n",
      "347 0.8872068524360657\n",
      "348 0.4394332766532898\n",
      "349 0.4383932054042816\n",
      "350 0.9254575967788696\n",
      "351 0.9253627061843872\n",
      "352 0.8858475685119629\n",
      "353 0.4338638186454773\n",
      "354 0.9250558018684387\n",
      "355 0.7674685716629028\n",
      "356 0.8848528861999512\n",
      "357 0.8845766186714172\n",
      "358 0.924625039100647\n",
      "359 0.42803630232810974\n",
      "360 0.8837165832519531\n",
      "361 0.9242847561836243\n",
      "362 0.4252292215824127\n",
      "363 0.9240577816963196\n",
      "364 0.4230806827545166\n",
      "365 0.42178934812545776\n",
      "366 0.9237462282180786\n",
      "367 0.7630024552345276\n",
      "368 0.41762781143188477\n",
      "369 0.7621942758560181\n",
      "370 0.9233565330505371\n",
      "371 0.7611435651779175\n",
      "372 0.8811044692993164\n",
      "373 0.7598767280578613\n",
      "374 0.8806127309799194\n",
      "375 0.8803247213363647\n",
      "376 0.9226972460746765\n",
      "377 0.8796966671943665\n",
      "378 0.4078671932220459\n",
      "379 0.40697765350341797\n",
      "380 0.7555557489395142\n",
      "381 0.9221035242080688\n",
      "382 0.4038059711456299\n",
      "383 0.8780117034912109\n",
      "384 0.9217697381973267\n",
      "385 0.9216492772102356\n",
      "386 0.9215210676193237\n",
      "387 0.39884310960769653\n",
      "388 0.8767408132553101\n",
      "389 0.7512418031692505\n",
      "390 0.876205563545227\n",
      "391 0.9208797216415405\n",
      "392 0.8755995035171509\n",
      "393 0.7492729425430298\n",
      "394 0.9204580187797546\n",
      "395 0.9203111529350281\n",
      "396 0.8743146657943726\n",
      "397 0.9200032949447632\n",
      "398 0.9198428988456726\n",
      "399 0.919677734375\n",
      "400 0.9195078015327454\n",
      "401 0.9193339347839355\n",
      "402 0.3903510868549347\n",
      "403 0.8722339868545532\n",
      "404 0.9188345074653625\n",
      "405 0.7441715598106384\n",
      "406 0.8713783025741577\n",
      "407 0.9183395504951477\n",
      "408 0.8707432746887207\n",
      "409 0.9179964065551758\n",
      "410 0.3870697617530823\n",
      "411 0.741657555103302\n",
      "412 0.9175054430961609\n",
      "413 0.7406517863273621\n",
      "414 0.9171836972236633\n",
      "415 0.9170181155204773\n",
      "416 0.8683185577392578\n",
      "417 0.8680006265640259\n",
      "418 0.3835391104221344\n",
      "419 0.8673283457756042\n",
      "420 0.8669735193252563\n",
      "421 0.8665845990180969\n",
      "422 0.8661662936210632\n",
      "423 0.38101381063461304\n",
      "424 0.7352768778800964\n",
      "425 0.7347180247306824\n",
      "426 0.8645385503768921\n",
      "427 0.7333514094352722\n",
      "428 0.37799954414367676\n",
      "429 0.8633283376693726\n",
      "430 0.8629210591316223\n",
      "431 0.3758321702480316\n",
      "432 0.9143184423446655\n",
      "433 0.37407031655311584\n",
      "434 0.3729747235774994\n",
      "435 0.371645987033844\n",
      "436 0.3701157569885254\n",
      "437 0.36840179562568665\n",
      "438 0.36652565002441406\n",
      "439 0.9135538339614868\n",
      "440 0.726311981678009\n",
      "441 0.9133642315864563\n",
      "442 0.3595978617668152\n",
      "443 0.35794907808303833\n",
      "444 0.7244469523429871\n",
      "445 0.8589636087417603\n",
      "446 0.8587113618850708\n",
      "447 0.3517359793186188\n",
      "448 0.7222346067428589\n",
      "449 0.9125145077705383\n",
      "450 0.720947265625\n",
      "451 0.9122546911239624\n",
      "452 0.7194751501083374\n",
      "453 0.34462302923202515\n",
      "454 0.9118317365646362\n",
      "455 0.9116874933242798\n",
      "456 0.3415524363517761\n",
      "457 0.8556286692619324\n",
      "458 0.7152988910675049\n",
      "459 0.8550226092338562\n",
      "460 0.910925030708313\n",
      "461 0.8543184399604797\n",
      "462 0.8539266586303711\n",
      "463 0.910382866859436\n",
      "464 0.9101861715316772\n",
      "465 0.3345779478549957\n",
      "466 0.7103830575942993\n",
      "467 0.9096158146858215\n",
      "468 0.3325210213661194\n",
      "469 0.7085763812065125\n",
      "470 0.9090790748596191\n",
      "471 0.8506304025650024\n",
      "472 0.8502564430236816\n",
      "473 0.8498388528823853\n",
      "474 0.8493820428848267\n",
      "475 0.9080827832221985\n",
      "476 0.7042428851127625\n",
      "477 0.7035468816757202\n",
      "478 0.9074277877807617\n",
      "479 0.907204270362854\n",
      "480 0.9069722890853882\n",
      "481 0.9067328572273254\n",
      "482 0.9064866900444031\n",
      "483 0.8454082608222961\n",
      "484 0.32525524497032166\n",
      "485 0.8445841073989868\n",
      "486 0.6978250741958618\n",
      "487 0.9052786827087402\n",
      "488 0.696536660194397\n",
      "489 0.8428646326065063\n",
      "490 0.6950355172157288\n",
      "491 0.6941677331924438\n",
      "492 0.8414624929428101\n",
      "493 0.6922486424446106\n",
      "494 0.6912019848823547\n",
      "495 0.8399559259414673\n",
      "496 0.6889765858650208\n",
      "497 0.9028912782669067\n",
      "498 0.8383892178535461\n",
      "499 0.6857215762138367\n"
     ]
    }
   ],
   "source": [
    "#Example of using the dynamic graph to construct variable sized model\n",
    "# Here we have a variable number of (re-used) middle hidden layer\n",
    "import random\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, hidden_size, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.dense_in = torch.nn.Linear(D_in, hidden_size)\n",
    "        self.dense_middle = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.dense_out = torch.nn.Linear(hidden_size, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_relu = torch.nn.ReLU()(self.dense_in(x))\n",
    "        for _ in range(random.randint(0,3)):\n",
    "            h_relu = torch.nn.ReLU()(self.dense_middle(h_relu))\n",
    "        y_pred = self.dense_out(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out))\n",
    "\n",
    "model = DynamicNet(D_in, hidden_size, D_out)\n",
    "model.cuda()\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x.cuda())\n",
    "    loss = criterion(y_pred, y.cuda())\n",
    "    print(epoch, loss.data[0])\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
